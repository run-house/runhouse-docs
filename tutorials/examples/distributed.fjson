{"parents": [], "prev": {"link": "../training/", "title": "Training: Transformers"}, "next": {"link": "../../../debugging_logging/", "title": "Debugging and Logging"}, "title": "Distributed: HF Accelerate", "meta": {}, "body": "<section id=\"distributed-hf-accelerate\">\n<h1>Distributed: HF Accelerate<a class=\"headerlink\" href=\"#distributed-hf-accelerate\" title=\"Permalink to this heading\">\u00b6</a></h1>\n<p><a href=\"https://colab.research.google.com/github/run-house/runhouse/blob/stable/docs/notebooks/examples/distributed.ipynb\">\n<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></p><p>This tutorial demonstrates how to use Runhouse with HuggingFace\naccelerate to launch distributed code on <strong>your own remote hardware</strong>.\nWe also show how one can reproducibly perform hardware dependency\nautosetup, to ensure that your code runs smoothly every time.</p>\n<p>You can run this on your own cluster, or through a standard cloud\naccount (AWS, GCP, Azure, LambdaLabs). If you do not have any compute or\ncloud accounts set up, we recommend creating a\n<a class=\"reference external\" href=\"https://cloud.lambdalabs.com/\">LambdaLabs</a> account for the easiest\nsetup path.</p>\n<section id=\"install-dependencies\">\n<h2>Install dependencies<a class=\"headerlink\" href=\"#install-dependencies\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"o\">!</span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>accelerate\n<span class=\"o\">!</span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>runhouse\n</pre></div>\n</div>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">runhouse</span> <span class=\"k\">as</span> <span class=\"nn\">rh</span>\n</pre></div>\n</div>\n<div class=\"code-output highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">17</span><span class=\"p\">:</span><span class=\"mi\">56</span><span class=\"p\">:</span><span class=\"mi\">13</span><span class=\"p\">,</span><span class=\"mi\">023</span> <span class=\"o\">|</span> <span class=\"n\">No</span> <span class=\"n\">auth</span> <span class=\"n\">token</span> <span class=\"n\">provided</span><span class=\"p\">,</span> <span class=\"n\">so</span> <span class=\"ow\">not</span> <span class=\"n\">using</span> <span class=\"n\">RNS</span> <span class=\"n\">API</span> <span class=\"n\">to</span> <span class=\"n\">save</span> <span class=\"ow\">and</span> <span class=\"n\">load</span> <span class=\"n\">configs</span>\n<span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">17</span><span class=\"p\">:</span><span class=\"mi\">56</span><span class=\"p\">:</span><span class=\"mi\">14</span><span class=\"p\">,</span><span class=\"mi\">334</span> <span class=\"o\">|</span> <span class=\"n\">NumExpr</span> <span class=\"n\">defaulting</span> <span class=\"n\">to</span> <span class=\"mi\">2</span> <span class=\"n\">threads</span><span class=\"o\">.</span>\n</pre></div>\n</div>\n</section>\n<section id=\"setting-up-the-cluster\">\n<h2>Setting up the Cluster<a class=\"headerlink\" href=\"#setting-up-the-cluster\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<section id=\"on-demand-cluster-aws-azure-gcp-or-lambdalabs\">\n<h3>On-Demand Cluster (AWS, Azure, GCP, or LambdaLabs)<a class=\"headerlink\" href=\"#on-demand-cluster-aws-azure-gcp-or-lambdalabs\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>For instructions on setting up cloud access for on-demand clusters,\nplease refer to <a class=\"reference external\" href=\"https://www.run.house/docs/tutorials/quick_start#cluster-setup\">Cluster\nSetup</a>.</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># single V100 GPU</span>\n<span class=\"c1\"># gpu = rh.ondemand_cluster(name=&quot;rh-v100&quot;, instance_type=&quot;V100:1&quot;).up_if_not()</span>\n\n<span class=\"c1\"># multigpu: 4 V100s</span>\n<span class=\"n\">gpu</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">ondemand_cluster</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;rh-4-v100&quot;</span><span class=\"p\">,</span> <span class=\"n\">instance_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;V100:4&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">up_if_not</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Set GPU to autostop after 60 min of inactivity (default is 30 min)</span>\n<span class=\"n\">gpu</span><span class=\"o\">.</span><span class=\"n\">keep_warm</span><span class=\"p\">(</span><span class=\"mi\">60</span><span class=\"p\">)</span>  <span class=\"c1\"># or -1 to keep up indefinitely</span>\n</pre></div>\n</div>\n</section>\n<section id=\"on-premise-cluster\">\n<h3>On-Premise Cluster<a class=\"headerlink\" href=\"#on-premise-cluster\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>For an on-prem cluster, you can instantaite it as follows, filling in\nthe IP address, ssh user and private key path.</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># For an existing cluster</span>\n<span class=\"c1\"># gpu = rh.cluster(ips=[&#39;&lt;ip of the cluster&gt;&#39;],</span>\n<span class=\"c1\">#                  ssh_creds={&#39;ssh_user&#39;: &#39;...&#39;, &#39;ssh_private_key&#39;:&#39;&lt;path_to_key&gt;&#39;},</span>\n<span class=\"c1\">#                  name=&#39;rh-cluster&#39;)</span>\n</pre></div>\n</div>\n</section>\n</section>\n<section id=\"setting-up-functions-on-remote-hardware\">\n<h2>Setting up Functions on Remote Hardware<a class=\"headerlink\" href=\"#setting-up-functions-on-remote-hardware\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<section id=\"training-function\">\n<h3>Training Function<a class=\"headerlink\" href=\"#training-function\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>For simplicity, let\u2019s use the\n<a class=\"reference external\" href=\"https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py#L114\">training_function</a>\nfrom\n<a class=\"reference external\" href=\"https://github.com/huggingface/accelerate/blob/v0.15.0/examples/nlp_example.py\">accelerate/examples/nlp_example.py</a>\nto demonstrate how to run this function remotely.</p>\n<p>In this case, because the function is available on GitHub, we can pass\nin a string pointing to the GitHub function.</p>\n<p>For local functions, for instance if we had <code class=\"docutils literal notranslate\"><span class=\"pre\">nlp_example.py</span></code> in our\ndirectory, we can also simply import the function.</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># if nlp_example.py is in local directory</span>\n<span class=\"c1\"># from nlp_example import training_function</span>\n\n<span class=\"c1\"># if function is available on GitHub, use it&#39;s string representation</span>\n<span class=\"n\">training_function</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;https://github.com/huggingface/accelerate/blob/v0.15.0/examples/nlp_example.py:training_function&quot;</span>\n</pre></div>\n</div>\n<p>Next, define the dependencies necessary to run the imported training\nfunction using accelerate.</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">reqs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;pip:./accelerate&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;transformers&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;datasets&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;evaluate&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;tqdm&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;scipy&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;scikit-learn&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tensorboard&#39;</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117&#39;</span><span class=\"p\">]</span>\n</pre></div>\n</div>\n<p>Now, we can put together the above components (gpu cluster, training\nfunction, and dependencies) to create our train function on remote\nhardware.</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">train_function_gpu</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span>\n                          <span class=\"n\">fn</span><span class=\"o\">=</span><span class=\"n\">training_function</span><span class=\"p\">,</span>\n                          <span class=\"n\">system</span><span class=\"o\">=</span><span class=\"n\">gpu</span><span class=\"p\">,</span>\n                          <span class=\"n\">reqs</span><span class=\"o\">=</span><span class=\"n\">reqs</span><span class=\"p\">,</span>\n                      <span class=\"p\">)</span>\n</pre></div>\n</div>\n<div class=\"code-output highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">21</span><span class=\"p\">:</span><span class=\"mi\">01</span><span class=\"p\">:</span><span class=\"mi\">46</span><span class=\"p\">,</span><span class=\"mi\">942</span> <span class=\"o\">|</span> <span class=\"n\">Setting</span> <span class=\"n\">up</span> <span class=\"n\">Function</span> <span class=\"n\">on</span> <span class=\"n\">cluster</span><span class=\"o\">.</span>\n<span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">21</span><span class=\"p\">:</span><span class=\"mi\">01</span><span class=\"p\">:</span><span class=\"mi\">46</span><span class=\"p\">,</span><span class=\"mi\">951</span> <span class=\"o\">|</span> <span class=\"n\">Installing</span> <span class=\"n\">packages</span> <span class=\"n\">on</span> <span class=\"n\">cluster</span> <span class=\"n\">rh</span><span class=\"o\">-</span><span class=\"n\">v100</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">&#39;GitPackage: https://github.com/huggingface/accelerate.git@v0.15.0&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;pip:./accelerate&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;transformers&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;datasets&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;evaluate&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tqdm&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;scipy&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;scikit-learn&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tensorboard&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">21</span><span class=\"p\">:</span><span class=\"mi\">02</span><span class=\"p\">:</span><span class=\"mi\">02</span><span class=\"p\">,</span><span class=\"mi\">988</span> <span class=\"o\">|</span> <span class=\"n\">Function</span> <span class=\"n\">setup</span> <span class=\"n\">complete</span><span class=\"o\">.</span>\n</pre></div>\n</div>\n<p><code class=\"docutils literal notranslate\"><span class=\"pre\">train_function_gpu</span></code> is a callable that can be used just like the\noriginal <code class=\"docutils literal notranslate\"><span class=\"pre\">training_function</span></code> function in the NLP example, except that\nit runs the function on the specified cluster/system instead.</p>\n</section>\n</section>\n<section id=\"launch-helper-function\">\n<h2>Launch Helper Function<a class=\"headerlink\" href=\"#launch-helper-function\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Here we define a helper function for launching accelerate training, and\nthen send the function to run on our GPU as well</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">launch_training</span><span class=\"p\">(</span><span class=\"n\">training_function</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">):</span>\n    <span class=\"kn\">from</span> <span class=\"nn\">accelerate.utils</span> <span class=\"kn\">import</span> <span class=\"n\">PrepareForLaunch</span><span class=\"p\">,</span> <span class=\"n\">patch_environment</span>\n    <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n    <span class=\"n\">num_processes</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">cuda</span><span class=\"o\">.</span><span class=\"n\">device_count</span><span class=\"p\">()</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s1\">&#39;Device count: </span><span class=\"si\">{</span><span class=\"n\">num_processes</span><span class=\"si\">}</span><span class=\"s1\">&#39;</span><span class=\"p\">)</span>\n    <span class=\"k\">with</span> <span class=\"n\">patch_environment</span><span class=\"p\">(</span><span class=\"n\">world_size</span><span class=\"o\">=</span><span class=\"n\">num_processes</span><span class=\"p\">,</span> <span class=\"n\">master_addr</span><span class=\"o\">=</span><span class=\"s2\">&quot;127.0.01&quot;</span><span class=\"p\">,</span> <span class=\"n\">master_port</span><span class=\"o\">=</span><span class=\"s2\">&quot;29500&quot;</span><span class=\"p\">,</span>\n                           <span class=\"n\">mixed_precision</span><span class=\"o\">=</span><span class=\"n\">args</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">mixed_precision</span><span class=\"p\">):</span>\n        <span class=\"n\">launcher</span> <span class=\"o\">=</span> <span class=\"n\">PrepareForLaunch</span><span class=\"p\">(</span><span class=\"n\">training_function</span><span class=\"p\">,</span> <span class=\"n\">distributed_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;MULTI_GPU&quot;</span><span class=\"p\">)</span>\n        <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">multiprocessing</span><span class=\"o\">.</span><span class=\"n\">start_processes</span><span class=\"p\">(</span><span class=\"n\">launcher</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">nprocs</span><span class=\"o\">=</span><span class=\"n\">num_processes</span><span class=\"p\">,</span> <span class=\"n\">start_method</span><span class=\"o\">=</span><span class=\"s2\">&quot;spawn&quot;</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">launch_training_gpu</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"o\">=</span><span class=\"n\">launch_training</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">gpu</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<div class=\"code-output highlight-default notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">19</span><span class=\"p\">:</span><span class=\"mi\">56</span><span class=\"p\">:</span><span class=\"mi\">15</span><span class=\"p\">,</span><span class=\"mi\">257</span> <span class=\"o\">|</span> <span class=\"n\">Writing</span> <span class=\"n\">out</span> <span class=\"n\">function</span> <span class=\"n\">function</span> <span class=\"n\">to</span> <span class=\"o\">/</span><span class=\"n\">content</span><span class=\"o\">/</span><span class=\"n\">launch_training_fn</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"k\">as</span> <span class=\"n\">functions</span> <span class=\"n\">serialized</span> <span class=\"ow\">in</span> <span class=\"n\">notebooks</span> <span class=\"n\">are</span> <span class=\"n\">brittle</span><span class=\"o\">.</span> <span class=\"n\">Please</span> <span class=\"n\">make</span> <span class=\"n\">sure</span> <span class=\"n\">the</span> <span class=\"n\">function</span> <span class=\"n\">does</span> <span class=\"ow\">not</span> <span class=\"n\">rely</span> <span class=\"n\">on</span> <span class=\"nb\">any</span> <span class=\"n\">local</span> <span class=\"n\">variables</span><span class=\"p\">,</span> <span class=\"n\">including</span> <span class=\"n\">imports</span> <span class=\"p\">(</span><span class=\"n\">which</span> <span class=\"n\">should</span> <span class=\"n\">be</span> <span class=\"n\">moved</span> <span class=\"n\">inside</span> <span class=\"n\">the</span> <span class=\"n\">function</span> <span class=\"n\">body</span><span class=\"p\">)</span><span class=\"o\">.</span>\n<span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">19</span><span class=\"p\">:</span><span class=\"mi\">56</span><span class=\"p\">:</span><span class=\"mi\">15</span><span class=\"p\">,</span><span class=\"mi\">262</span> <span class=\"o\">|</span> <span class=\"n\">Setting</span> <span class=\"n\">up</span> <span class=\"n\">Function</span> <span class=\"n\">on</span> <span class=\"n\">cluster</span><span class=\"o\">.</span>\n<span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">19</span><span class=\"p\">:</span><span class=\"mi\">56</span><span class=\"p\">:</span><span class=\"mi\">15</span><span class=\"p\">,</span><span class=\"mi\">265</span> <span class=\"o\">|</span> <span class=\"n\">Copying</span> <span class=\"n\">local</span> <span class=\"n\">package</span> <span class=\"n\">content</span> <span class=\"n\">to</span> <span class=\"n\">cluster</span> <span class=\"o\">&lt;</span><span class=\"n\">rh</span><span class=\"o\">-</span><span class=\"n\">v100</span><span class=\"o\">&gt;</span>\n<span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">19</span><span class=\"p\">:</span><span class=\"mi\">56</span><span class=\"p\">:</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">623</span> <span class=\"o\">|</span> <span class=\"n\">Installing</span> <span class=\"n\">packages</span> <span class=\"n\">on</span> <span class=\"n\">cluster</span> <span class=\"n\">rh</span><span class=\"o\">-</span><span class=\"n\">v100</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s1\">&#39;./&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">INFO</span> <span class=\"o\">|</span> <span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">03</span><span class=\"o\">-</span><span class=\"mi\">20</span> <span class=\"mi\">19</span><span class=\"p\">:</span><span class=\"mi\">56</span><span class=\"p\">:</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">753</span> <span class=\"o\">|</span> <span class=\"n\">Function</span> <span class=\"n\">setup</span> <span class=\"n\">complete</span><span class=\"o\">.</span>\n</pre></div>\n</div>\n</section>\n<section id=\"launch-distributed-training\">\n<h2>Launch Distributed Training<a class=\"headerlink\" href=\"#launch-distributed-training\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Now, we\u2019re ready to launch distributed training on our self-hosted\nhardware!</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">argparse</span>\n\n<span class=\"c1\"># define basic train args and hyperparams</span>\n<span class=\"n\">train_args</span> <span class=\"o\">=</span> <span class=\"n\">argparse</span><span class=\"o\">.</span><span class=\"n\">Namespace</span><span class=\"p\">(</span><span class=\"n\">cpu</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"n\">mixed_precision</span><span class=\"o\">=</span><span class=\"s1\">&#39;fp16&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">hps</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&quot;lr&quot;</span><span class=\"p\">:</span> <span class=\"mf\">2e-5</span><span class=\"p\">,</span> <span class=\"s2\">&quot;num_epochs&quot;</span><span class=\"p\">:</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s2\">&quot;seed&quot;</span><span class=\"p\">:</span> <span class=\"mi\">42</span><span class=\"p\">,</span> <span class=\"s2\">&quot;batch_size&quot;</span><span class=\"p\">:</span> <span class=\"mi\">16</span><span class=\"p\">}</span>\n</pre></div>\n</div>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">launch_training_gpu</span><span class=\"p\">(</span><span class=\"n\">train_function_gpu</span><span class=\"p\">,</span> <span class=\"n\">hps</span><span class=\"p\">,</span> <span class=\"n\">train_args</span><span class=\"p\">,</span> <span class=\"n\">stream_logs</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<pre class=\"code-output literal-block\">INFO | 2023-03-20 20:11:45,415 | Running launch_training via gRPC\nINFO | 2023-03-20 20:11:45,718 | Time to send message: 0.3 seconds\nINFO | 2023-03-20 20:11:45,720 | Submitted remote call to cluster. Result or logs can be retrieved\n with run_key &quot;launch_training_20230320_201145&quot;, e.g.\n<cite>rh.cluster(name=&quot;~/rh-v100&quot;).get(&quot;launch_training_20230320_201145&quot;, stream_logs=True)</cite> in python\n<cite>runhouse logs &quot;rh-v100&quot; launch_training_20230320_201145</cite> from the command line.\n or cancelled with\n<cite>rh.cluster(name=&quot;~/rh-v100&quot;).cancel(&quot;launch_training_20230320_201145&quot;)</cite> in python or\n<cite>runhouse cancel &quot;rh-v100&quot; launch_training_20230320_201145</cite> from the command line.\n:task_name:launch_training\n:task_name:launch_training\nINFO | 2023-03-20 20:11:46,328 | Loading config from local file /home/ubuntu/runhouse/runhouse/builtins/config.json\nINFO | 2023-03-20 20:11:46,328 | No auth token provided, so not using RNS API to save and load configs\nDevice count: 1\nINFO | 2023-03-20 20:11:49,486 | Loading config from local file /home/ubuntu/runhouse/runhouse/builtins/config.json\nINFO | 2023-03-20 20:11:49,486 | No auth token provided, so not using RNS API to save and load configs\nINFO | 2023-03-20 20:11:49,844 | Appending /home/ubuntu/accelerate/examples to sys.path\nINFO | 2023-03-20 20:11:49,844 | Importing module nlp_example\n\nDownloading builder script:   0%|          | 0.00/5.75k [00:00&lt;?, ?B/s]\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.75k/5.75k [00:00&lt;00:00, 8.19MB/s]\n\nDownloading (\u2026)okenizer_config.json:   0%|          | 0.00/29.0 [00:00&lt;?, ?B/s]\nDownloading (\u2026)okenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.0/29.0 [00:00&lt;00:00, 65.9kB/s]\n\nDownloading (\u2026)lve/main/config.json:   0%|          | 0.00/570 [00:00&lt;?, ?B/s]\nDownloading (\u2026)lve/main/config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 570/570 [00:00&lt;00:00, 1.22MB/s]\n\nDownloading (\u2026)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00&lt;?, ?B/s]\nDownloading (\u2026)solve/main/vocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 213k/213k [00:00&lt;00:00, 1.05MB/s]\nDownloading (\u2026)solve/main/vocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 213k/213k [00:00&lt;00:00, 1.05MB/s]\n\nDownloading (\u2026)/main/tokenizer.json:   0%|          | 0.00/436k [00:00&lt;?, ?B/s]\nDownloading (\u2026)/main/tokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 436k/436k [00:00&lt;00:00, 1.61MB/s]\nDownloading (\u2026)/main/tokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 436k/436k [00:00&lt;00:00, 1.60MB/s]\n\nDownloading builder script:   0%|          | 0.00/28.8k [00:00&lt;?, ?B/s]\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.8k/28.8k [00:00&lt;00:00, 380kB/s]\n\nDownloading metadata:   0%|          | 0.00/28.7k [00:00&lt;?, ?B/s]\nDownloading metadata: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.7k/28.7k [00:00&lt;00:00, 422kB/s]\n\nDownloading readme:   0%|          | 0.00/27.9k [00:00&lt;?, ?B/s]\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 27.9k/27.9k [00:00&lt;00:00, 412kB/s]\n\nDownloading data files:   0%|          | 0/3 [00:00&lt;?, ?it/s]Downloading and preparing dataset glue/mrpc to /home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n\n\nDownloading data: 0.00B [00:00, ?B/s]\u001b[A\nDownloading data: 6.22kB [00:00, 11.1MB/s]\n\nDownloading data files:  33%|\u2588\u2588\u2588\u258e      | 1/3 [00:00&lt;00:00,  4.26it/s]\n\nDownloading data: 0.00B [00:00, ?B/s]\u001b[A\nDownloading data: 1.05MB [00:00, 55.0MB/s]\n\nDownloading data files:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 2/3 [00:00&lt;00:00,  5.30it/s]\n\nDownloading data: 0.00B [00:00, ?B/s]\u001b[A\nDownloading data: 441kB [00:00, 44.3MB/s]\n\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  5.87it/s]\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  5.56it/s]\n\nGenerating train split:   0%|          | 0/3668 [00:00&lt;?, ? examples/s]\nGenerating train split:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 2898/3668 [00:00&lt;00:00, 28934.98 examples/s]\n\n\nGenerating validation split:   0%|          | 0/408 [00:00&lt;?, ? examples/s]\n\n\nGenerating test split:   0%|          | 0/1725 [00:00&lt;?, ? examples/s]\n\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 1296.81it/s]\n\nMap:   0%|          | 0/3668 [00:00&lt;?, ? examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3668/3668 [00:00&lt;00:00, 33587.18 examples/s]\n\n\nMap:   0%|          | 0/408 [00:00&lt;?, ? examples/s]\n\n\nMap:   0%|          | 0/1725 [00:00&lt;?, ? examples/s]\n\nDataset glue downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n\nDownloading pytorch_model.bin:   0%|          | 0.00/436M [00:00&lt;?, ?B/s]\nDownloading pytorch_model.bin:   2%|\u258f         | 10.5M/436M [00:00&lt;00:04, 95.8MB/s]\nDownloading pytorch_model.bin:   5%|\u258d         | 21.0M/436M [00:00&lt;00:04, 97.1MB/s]\nDownloading pytorch_model.bin:   7%|\u258b         | 31.5M/436M [00:00&lt;00:04, 93.2MB/s]\nDownloading pytorch_model.bin:  10%|\u2589         | 41.9M/436M [00:00&lt;00:04, 91.3MB/s]\nDownloading pytorch_model.bin:  12%|\u2588\u258f        | 52.4M/436M [00:00&lt;00:04, 92.6MB/s]\nDownloading pytorch_model.bin:  14%|\u2588\u258d        | 62.9M/436M [00:00&lt;00:04, 86.0MB/s]\nDownloading pytorch_model.bin:  17%|\u2588\u258b        | 73.4M/436M [00:00&lt;00:04, 89.9MB/s]\nDownloading pytorch_model.bin:  19%|\u2588\u2589        | 83.9M/436M [00:00&lt;00:03, 90.2MB/s]\nDownloading pytorch_model.bin:  22%|\u2588\u2588\u258f       | 94.4M/436M [00:01&lt;00:03, 91.5MB/s]\nDownloading pytorch_model.bin:  24%|\u2588\u2588\u258d       | 105M/436M [00:01&lt;00:03, 93.3MB/s]\nDownloading pytorch_model.bin:  26%|\u2588\u2588\u258b       | 115M/436M [00:01&lt;00:03, 86.5MB/s]\nDownloading pytorch_model.bin:  29%|\u2588\u2588\u2589       | 126M/436M [00:01&lt;00:03, 86.9MB/s]\nDownloading pytorch_model.bin:  31%|\u2588\u2588\u2588\u258f      | 136M/436M [00:01&lt;00:03, 87.2MB/s]\nDownloading pytorch_model.bin:  34%|\u2588\u2588\u2588\u258e      | 147M/436M [00:01&lt;00:03, 88.6MB/s]\nDownloading pytorch_model.bin:  36%|\u2588\u2588\u2588\u258c      | 157M/436M [00:01&lt;00:03, 90.7MB/s]\nDownloading pytorch_model.bin:  38%|\u2588\u2588\u2588\u258a      | 168M/436M [00:01&lt;00:02, 90.4MB/s]\nDownloading pytorch_model.bin:  41%|\u2588\u2588\u2588\u2588      | 178M/436M [00:02&lt;00:03, 82.5MB/s]\nDownloading pytorch_model.bin:  43%|\u2588\u2588\u2588\u2588\u258e     | 189M/436M [00:02&lt;00:02, 84.6MB/s]\nDownloading pytorch_model.bin:  46%|\u2588\u2588\u2588\u2588\u258c     | 199M/436M [00:02&lt;00:02, 81.3MB/s]\nDownloading pytorch_model.bin:  48%|\u2588\u2588\u2588\u2588\u258a     | 210M/436M [00:02&lt;00:02, 84.4MB/s]\nDownloading pytorch_model.bin:  51%|\u2588\u2588\u2588\u2588\u2588     | 220M/436M [00:02&lt;00:02, 83.4MB/s]\nDownloading pytorch_model.bin:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 231M/436M [00:02&lt;00:02, 86.4MB/s]\nDownloading pytorch_model.bin:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 241M/436M [00:02&lt;00:02, 88.9MB/s]\nDownloading pytorch_model.bin:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 252M/436M [00:02&lt;00:02, 90.9MB/s]\nDownloading pytorch_model.bin:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 262M/436M [00:02&lt;00:01, 91.6MB/s]\nDownloading pytorch_model.bin:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 273M/436M [00:03&lt;00:01, 90.9MB/s]\nDownloading pytorch_model.bin:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 283M/436M [00:03&lt;00:01, 90.8MB/s]\nDownloading pytorch_model.bin:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 294M/436M [00:03&lt;00:01, 91.6MB/s]\nDownloading pytorch_model.bin:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 304M/436M [00:03&lt;00:01, 92.1MB/s]\nDownloading pytorch_model.bin:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 315M/436M [00:03&lt;00:01, 91.9MB/s]\nDownloading pytorch_model.bin:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 325M/436M [00:03&lt;00:01, 91.0MB/s]\nDownloading pytorch_model.bin:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 336M/436M [00:03&lt;00:01, 89.7MB/s]\nDownloading pytorch_model.bin:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 346M/436M [00:03&lt;00:00, 90.2MB/s]\nDownloading pytorch_model.bin:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 357M/436M [00:03&lt;00:00, 92.1MB/s]\nDownloading pytorch_model.bin:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 367M/436M [00:04&lt;00:00, 93.5MB/s]\nDownloading pytorch_model.bin:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 377M/436M [00:04&lt;00:00, 93.5MB/s]\nDownloading pytorch_model.bin:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 388M/436M [00:04&lt;00:00, 92.9MB/s]\nDownloading pytorch_model.bin:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 398M/436M [00:04&lt;00:00, 81.5MB/s]\nDownloading pytorch_model.bin:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 409M/436M [00:04&lt;00:00, 83.7MB/s]\nDownloading pytorch_model.bin:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 419M/436M [00:04&lt;00:00, 85.6MB/s]\nDownloading pytorch_model.bin:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 430M/436M [00:04&lt;00:00, 80.6MB/s]\nDownloading pytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 436M/436M [00:04&lt;00:00, 88.2MB/s]\nSome weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the <cite>__call__</cite> method is faster than using a method to encode the text followed by a call to the <cite>pad</cite> method to get a padded encoding.\nepoch 0: {'accuracy': 0.7745098039215687, 'f1': 0.8557993730407523}\nepoch 1: {'accuracy': 0.8406862745098039, 'f1': 0.8849557522123894}\nepoch 2: {'accuracy': 0.8553921568627451, 'f1': 0.8981001727115717}</pre>\n</section>\n<section id=\"terminate-cluster\">\n<h2>Terminate Cluster<a class=\"headerlink\" href=\"#terminate-cluster\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Once you are done using the cluster, you can terminate it as follows:</p>\n<div class=\"highlight-ipython3 notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">gpu</span><span class=\"o\">.</span><span class=\"n\">teardown</span><span class=\"p\">()</span>\n</pre></div>\n</div>\n</section>\n</section>\n\n    <script type=\"text/x-thebe-config\">\n    {\n        requestKernel: true,\n        binderOptions: {\n            repo: \"binder-examples/jupyter-stacks-datascience\",\n            ref: \"master\",\n        },\n        codeMirrorConfig: {\n            theme: \"abcdef\",\n            mode: \"python\"\n        },\n        kernelOptions: {\n            kernelName: \"python3\",\n            path: \"./tutorials/examples\"\n        },\n        predefinedOutput: true\n    }\n    </script>\n    <script>kernelName = 'python3'</script>", "metatags": "<meta name=\"generator\" content=\"Docutils 0.18.1: http://docutils.sourceforge.net/\" />\n", "rellinks": [["genindex", "General Index", "I", "index"], ["py-modindex", "Python Module Index", "", "modules"], ["debugging_logging", "Debugging and Logging", "N", "next"], ["tutorials/examples/training", "Training: Transformers", "P", "previous"]], "sourcename": "tutorials/examples/distributed.rst.txt", "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Distributed: HF Accelerate</a><ul>\n<li><a class=\"reference internal\" href=\"#install-dependencies\">Install dependencies</a></li>\n<li><a class=\"reference internal\" href=\"#setting-up-the-cluster\">Setting up the Cluster</a><ul>\n<li><a class=\"reference internal\" href=\"#on-demand-cluster-aws-azure-gcp-or-lambdalabs\">On-Demand Cluster (AWS, Azure, GCP, or LambdaLabs)</a></li>\n<li><a class=\"reference internal\" href=\"#on-premise-cluster\">On-Premise Cluster</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#setting-up-functions-on-remote-hardware\">Setting up Functions on Remote Hardware</a><ul>\n<li><a class=\"reference internal\" href=\"#training-function\">Training Function</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#launch-helper-function\">Launch Helper Function</a></li>\n<li><a class=\"reference internal\" href=\"#launch-distributed-training\">Launch Distributed Training</a></li>\n<li><a class=\"reference internal\" href=\"#terminate-cluster\">Terminate Cluster</a></li>\n</ul>\n</li>\n</ul>\n", "display_toc": true, "page_source_suffix": ".rst", "globaltoc": "<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick_start/\">Quick Start Guide</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"architecture/\">Architecture Overview</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api_tutorials/\">API Tutorials</a><ul>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tutorials/api/compute/\">Compute: Clusters, Functions, Packages, &amp; Envs</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tutorials/api/data/\">Data: Folders, Tables, &amp; Blobs</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"tutorials/api/accessibility/\">Accessibility: Resource and Secrets Management</a></li>\n</ul>\n</li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Reference</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/python/\">Python API</a><ul>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/resource/\">Resource</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/function/\">Function</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/cluster/\">Cluster</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/env/\">Env</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/package/\">Package</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/run/\">Run</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/module/\">Module</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/folder/\">Folder</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/table/\">Table</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/blob/\">Blob</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/secrets/\">Secrets</a></li>\n</ul>\n</li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/cli/\">Command Line Interface</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Usage Examples</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/examples/inference/\">Inference: Stable Diffusion and FLAN-T5</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/examples/training/\">Training: Transformers</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/examples/distributed/\">Distributed: HF Accelerate</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://github.com/run-house/funhouse/tree/main/bert_pipeline\">Pipelining: BERT</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Additional Resources</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"debugging_logging/\">Debugging and Logging</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"troubleshooting/\">Manual Setup and Troubleshooting</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"data_collection/\">Security and Metadata Collection</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://github.com/run-house/runhouse\">Source Code</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://api.run.house/docs\">REST API Guide</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://www.run.house/dashboard\">Dashboard</a></li>\n<li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://github.com/run-house/funhouse\">Funhouse</a></li>\n</ul>\n", "current_page_name": "tutorials/examples/distributed", "sidebars": ["about.html", "navigation.html", "relations.html", "searchbox.html", "donate.html"], "customsidebar": null, "favicon_url": null, "logo_url": null, "alabaster_version": "0.7.12"}