{"parents": [], "prev": {"link": "../tutorials/quick-start-den/", "title": "Den Quick Start"}, "next": {"link": "../runhouse-in-your-stack/", "title": "Working with Common Libraries and Tools"}, "title": "How to Use Runhouse", "meta": {}, "body": "<section id=\"how-to-use-runhouse\">\n<h1>How to Use Runhouse<a class=\"headerlink\" href=\"#how-to-use-runhouse\" title=\"Permalink to this heading\">\u00b6</a></h1>\n<p>This page offers a more detailed guide on using Runhouse to develop and deploy your ML projects. If you have any\nquestions about what is described here, please reach out to <a class=\"reference external\" href=\"mailto:hello&#37;&#52;&#48;run&#46;house\">hello<span>&#64;</span>run<span>&#46;</span>house</a> or ping us on\n<a class=\"reference external\" href=\"https://discord.gg/RnhB6589Hs\">Discord</a>, and we\u2019d be happy to walk you through the details.</p>\n<section id=\"quick-start\">\n<h2>Quick Start<a class=\"headerlink\" href=\"#quick-start\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Before reviewing this detailed guide, we recommend you start with the <a class=\"reference external\" href=\"https://www.run.house/docs/tutorials/quick-start-cloud\">Quick Start</a> guide.</p>\n<ul class=\"simple\">\n<li><p>Install Runhouse with <code class=\"docutils literal notranslate\"><span class=\"pre\">pip</span> <span class=\"pre\">install</span> <span class=\"pre\">runhouse</span></code></p></li>\n<li><p>Optionally install with a specific cloud like <code class=\"docutils literal notranslate\"><span class=\"pre\">pip</span> <span class=\"pre\">install</span> <span class=\"pre\">&quot;runhouse[aws]&quot;</span></code> or with\n<a class=\"reference external\" href=\"https://skypilot.readthedocs.io/en/latest/docs/index.html\">SkyPilot</a> for elastic compute\n<code class=\"docutils literal notranslate\"><span class=\"pre\">pip</span> <span class=\"pre\">install</span> <span class=\"pre\">&quot;runhouse[sky]&quot;</span></code></p></li>\n<li><p>Optionally create an account on the <a class=\"reference external\" href=\"https://www.run.house/dashboard\">Runhouse website</a> or with\n<code class=\"docutils literal notranslate\"><span class=\"pre\">runhouse</span> <span class=\"pre\">login</span> <span class=\"pre\">--sync-secrets</span></code> to enable saving, reloading, and centralized authentication / secrets management.</p></li>\n</ul>\n</section>\n<section id=\"access-to-compute\">\n<h2>Access to Compute<a class=\"headerlink\" href=\"#access-to-compute\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>In order to use Runhouse, you must be able to access compute resources, which can take any form (e.g. VMs, elastic\ncompute, Kubernetes). You should think about all the compute resources you have as a single pool, from which Runhouse\nallows you to launch ephemeral clusters to execute your code.</p>\n<ul class=\"simple\">\n<li><p><strong>Kubernetes</strong>: All you need is a kubeconfig</p></li>\n<li><p><strong>Elastic Compute</strong>: We use Skypilot under the hood to launch elastic compute, and support most clouds. You can run\n<code class=\"docutils literal notranslate\"><span class=\"pre\">sky</span> <span class=\"pre\">check</span></code> in CLI after installing <code class=\"docutils literal notranslate\"><span class=\"pre\">runhouse[sky]</span></code> to confirm you have access to the cloud, or for the necessary\nsetup steps.</p></li>\n<li><p><strong>Existing Clusters</strong>: Runhouse supports a variety of authentication methods to access existing clusters, including\nSSH with keys or passwords.</p></li>\n</ul>\n<p>For initial projects and getting started quickly, launching from local credentials is possible. In this setting, you\nalready unlock serverless execution for your Python ML code, but you cannot yet take advantage of advanced usage\npatterns from compute saving and reuse.</p>\n<p>For production settings, we recommend that users load cloud secrets, Kubeconfig, and available compute resources into\nRunhouse Den and authenticate from all launch environments using only the Runhouse token. Platform teams gain\ncentralized observability over utilization, including insights into who is launching clusters, how often they are\nlaunched, and the resources or tasks executed on them. Access management becomes much simpler, especially in\nmulti-cloud or multi-cluster environments. To get started with Den enabled, simply run\n<code class=\"docutils literal notranslate\"><span class=\"pre\">runhouse</span> <span class=\"pre\">login</span> <span class=\"pre\">--sync-secrets</span></code> in the CLI.</p>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>If you do not have access to the cloud from a local machine (ex: from within an orchestration pipeline), you\ncan use the Runhouse control plane to launch your compute. See <a class=\"reference internal\" href=\"../tutorials/quick-start-den/#den-launcher\"><span class=\"std std-ref\">Den Launcher</span></a> for more information.</p>\n</div>\n</section>\n<section id=\"start-your-project\">\n<h2>Start Your Project<a class=\"headerlink\" href=\"#start-your-project\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Once you have established access to compute, you can start developing a new ML project. The following steps will\nprovide the details of how to use Runhouse, starting from a blank page in your IDE.</p>\n<section id=\"define-compute\">\n<h3>1. Define Compute<a class=\"headerlink\" href=\"#define-compute\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>Runhouse allows you to define compute requirements in code, and launch ephemeral clusters from the compute pool we\ndescribed in the prior section. Here, you can define the required CPU, GPU, memory, and disk requirements (or name\na specific cluster) to use.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/Pull+Compute+from+Compute+Pool.jpg\"><img alt=\"Runhouse pulls compute from a pool of resources\" class=\"align-center\" src=\"https://runhouse-tutorials.s3.amazonaws.com/Pull+Compute+from+Compute+Pool.jpg\" style=\"width: 750px;\" /></a>\n<p>For instance, to create a cluster on AWS with an A10 GPU attached, you can write:</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">runhouse</span> <span class=\"k\">as</span> <span class=\"nn\">rh</span>\n\n<span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">ondemand_cluster</span><span class=\"p\">(</span>\n    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;rh-cluster&quot;</span><span class=\"p\">,</span> <span class=\"c1\"># This cluster can be saved and reused by name. We will prefix your username when saved, e.g. /my_username/rh-cluster</span>\n    <span class=\"n\">instance_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;A10G:1&quot;</span><span class=\"p\">,</span> <span class=\"c1\"># There are a number of options available for instance_type, check out the docs to see them all</span>\n    <span class=\"n\">provider</span><span class=\"o\">=</span><span class=\"s2\">&quot;aws&quot;</span><span class=\"p\">,</span> <span class=\"c1\"># Specify a cloud provider</span>\n    <span class=\"n\">autostop_mins</span><span class=\"o\">=</span><span class=\"mi\">90</span><span class=\"p\">,</span> <span class=\"c1\"># Remember to set autostop_mins to avoid leaving clusters running indefinitely.</span>\n    <span class=\"n\">launcher</span><span class=\"o\">=</span><span class=\"s2\">&quot;den&quot;</span><span class=\"p\">,</span> <span class=\"c1\"># Launch the cluster with Runhouse; use &#39;local&#39; for local credentials</span>\n<span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">up_if_not</span><span class=\"p\">()</span>\n</pre></div>\n</div>\n<p>You can also define a Runhouse Image containing a base machine or Docker image, along with other setup steps (e.g.\npackage, installs, bash commands, env vars), and pass it into the factory function above to specify cluster setup prior\nto starting Runhouse on the cluster.</p>\n<p>After the cluster is up, you can also run CLI commands on the cluster using <code class=\"docutils literal notranslate\"><span class=\"pre\">cluster.run_bash()</span></code> to run additional\nsetup commands.</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">cluster</span><span class=\"o\">.</span><span class=\"n\">run_bash</span><span class=\"p\">([</span><span class=\"s1\">&#39;pip install numpy&#39;</span><span class=\"p\">])</span>\n</pre></div>\n</div>\n<p>You can find full documentation about the Runhouse cluster API in the <a class=\"reference external\" href=\"https://www.run.house/docs/tutorials/api-clusters\">Cluster docs</a>.</p>\n<section id=\"starting-the-runhouse-server-daemon\">\n<h4>Starting the Runhouse Server Daemon<a class=\"headerlink\" href=\"#starting-the-runhouse-server-daemon\" title=\"Permalink to this heading\">\u00b6</a></h4>\n<p>If not already running, the client will start the Runhouse API server daemon on the compute and form a secure network\nconnection (either over SSH or HTTP/S).</p>\n<ul class=\"simple\">\n<li><p>The daemon can be thought of as a \u201cPython object store,\u201d holding key-value pairs of names and Python objects in\nmemory (objects you will dispatch to it in the next step), and exposing an HTTP API to call methods on those\nobjects by name.</p></li>\n<li><p>By default, objects are held in a single default worker process but can be sent to other worker processes, including\non other nodes in the cluster, to achieve powerful parallelism out of the box.</p></li>\n<li><p>When the object is used, there is a <code class=\"docutils literal notranslate\"><span class=\"pre\">GET</span> <span class=\"pre\">http://myserver:32300/my_object/my_method</span></code>, and the daemon will look up\nthe object named \u201cmy_object,\u201d issue an instruction for its worker to call the method \u201cmy_method\u201d on it, and return\nthe result.</p></li>\n<li><p>The HTTP server and workers can handle thousands of concurrent calls per second, and have similar latency to Flask\nunder most conditions.</p></li>\n</ul>\n</section>\n</section>\n<section id=\"dispatch-your-code\">\n<h3>2. Dispatch Your Code<a class=\"headerlink\" href=\"#dispatch-your-code\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>Once you have established a connection to compute, the development pattern is to continuously dispatch code to the\ncluster and execute it there. You are doing local-like execution and debug, but with the power of the remote compute.\nRunhouse is agnostic to whether you dispatch using a Notebook or run directly from a Python script.</p>\n<p>Specifically to do the dispatch, you wrap your local function with <code class=\"docutils literal notranslate\"><span class=\"pre\">rh.function()</span></code> or class with <code class=\"docutils literal notranslate\"><span class=\"pre\">rh.module()</span></code>,\nand send it to the cluster with <code class=\"docutils literal notranslate\"><span class=\"pre\">.to(cluster)</span></code>. For functions, you can call them directly as if they were local\nfunctions, and they run remotely. For modules, you instantiate a remote instance of the object which is stateful;\nyou can access this remote object by name and make multi-threaded calls to its methods.</p>\n<p>When you <code class=\"docutils literal notranslate\"><span class=\"pre\">.to()</span></code> a local function or class to the cluster, the corresponding repository or package, along with any\nlocal dependencies, is rsynced to the cluster. An instruction containing the import path is then sent to the cluster to\nconstruct the function or class in a specific worker, and it is inserted into the key-value store. We avoid and\ndiscourage serializing code, as serialization often leads to version mismatch errors between local and remote package\nversions.</p>\n<p>After the object is deployed to the server, the Runhouse Python client returns a local callable stub. It behaves like\nthe original object but forwards method calls over HTTP to the remote object on the cluster.</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"k\">def</span> <span class=\"nf\">add_two_numbers</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span><span class=\"n\">b</span><span class=\"p\">):</span>\n      <span class=\"k\">return</span> <span class=\"n\">a</span><span class=\"o\">+</span><span class=\"n\">b</span>\n\n<span class=\"n\">remote_add</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">add_two_numbers</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"p\">)</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TorchTrainer</span><span class=\"p\">:</span>\n   <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n      <span class=\"o\">..</span>\n\n   <span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n      <span class=\"o\">..</span>\n\n   <span class=\"k\">def</span> <span class=\"nf\">test</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n      <span class=\"o\">..</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&quot;__main__&quot;</span><span class=\"p\">:</span>\n   <span class=\"n\">cluster</span><span class=\"o\">.</span><span class=\"n\">install_packages</span><span class=\"p\">([</span><span class=\"s2\">&quot;torch&quot;</span><span class=\"p\">])</span>\n   <span class=\"n\">RemoteTrainer</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"p\">(</span><span class=\"n\">TorchTrainer</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"p\">)</span> <span class=\"c1\"># Send to cluster</span>\n   <span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">RemoteTrainer</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;remote-instance-of-trainer&#39;</span><span class=\"p\">)</span> <span class=\"c1\"># Instantiate remote object</span>\n</pre></div>\n</div>\n<div class=\"admonition note\">\n<p class=\"admonition-title\">Note</p>\n<p>The code that should only run locally (e.g. defining compute, dispatch, and calling remote objects for execution)\nshould live within a <code class=\"docutils literal notranslate\"><span class=\"pre\">if</span> <span class=\"pre\">__name__</span> <span class=\"pre\">==</span> <span class=\"pre\">&quot;__main__&quot;:</span></code> block in a script. This way, the code will not execute on remote compute\nwhen it is sent there.</p>\n</div>\n<p>Read more about <a class=\"reference external\" href=\"https://www.run.house/docs/tutorials/api-modules\">functions and modules</a>.</p>\n</section>\n<section id=\"execute-your-code-remotely\">\n<h3>3. Execute Your Code Remotely<a class=\"headerlink\" href=\"#execute-your-code-remotely\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>It\u2019s now possible to use your remote objects as if they were local. From here on, you can think of Runhouse as\nfacilitating regular object-oriented programming but with the objects living remotely, maybe in a different cluster,\nregion, or cloud than the local code. Python behavior such as async, exceptions, printing, and logging are all\npreserved across remote calls, but can also be disabled or controlled if desired.</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">remote_add</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"o\">...</span>  <span class=\"c1\"># Load data</span>\n<span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>As noted above, you should be iteratively dispatching and executing code. If you make local updates to the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">add_two_numbers</span></code> function or the <code class=\"docutils literal notranslate\"><span class=\"pre\">TorchTrainer</span></code> class, you can simply re-run <code class=\"docutils literal notranslate\"><span class=\"pre\">.to()</span></code>, and it should take &lt;2\nseconds to redeploy. The underlying cluster is persisted and stateful until you choose to down it, so you can take\nadvantage of the remote file system and memory during interactive development as well.</p>\n<p>These remote objects are accessible from anywhere you are authenticated with Runhouse, so you and your team can make\nmulti-threaded calls against them. Calling microservices is actually a familiar pattern in programming; however, no\nteam would ever manually split their ML pipeline into multiple applications due to the DevOps overhead.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/Iterative+Dispatch+from+Notebook.jpg\"><img alt=\"Iteratively develop and dispatch code to remote execution\" class=\"align-center\" src=\"https://runhouse-tutorials.s3.amazonaws.com/Iterative+Dispatch+from+Notebook.jpg\" style=\"width: 550px;\" /></a>\n</section>\n<section id=\"saving-and-loading\">\n<h3>4. Saving and Loading<a class=\"headerlink\" href=\"#saving-and-loading\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>Runhouse resources (clusters, functions, modules) can be saved, shared, and reused based on a compact JSON metadata\nsignature. This allows for easy sharing of clusters and services across users and environments. For instance, the team\nmight want to use a single shared embeddings service to save costs and improve reproducibility.</p>\n<p>Runhouse comes with a built-in metadata store / service registry called <a class=\"reference external\" href=\"https://www.run.house/dashboard\">Den</a> to\nfacilitate convenient saving, loading, sharing, and management of these resources. Den can be accessed via an HTTP\nAPI or from any Python interpreter with a Runhouse token (either in <code class=\"docutils literal notranslate\"><span class=\"pre\">~/.rh/config.yaml</span></code> or an <code class=\"docutils literal notranslate\"><span class=\"pre\">RH_TOKEN</span></code>\nenvironment variable):</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Save to Den</span>\n<span class=\"n\">remote_add</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;my_function&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Reload the function and invoke it remotely on the cluster</span>\n<span class=\"n\">my_func</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;/my_username/my_function&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Share the function with another user, giving them access to call or modify the resource</span>\n<span class=\"n\">my_func</span><span class=\"o\">.</span><span class=\"n\">share</span><span class=\"p\">(</span><span class=\"s2\">&quot;user_a@gmail.com&quot;</span><span class=\"p\">,</span> <span class=\"n\">access_level</span><span class=\"o\">=</span><span class=\"s2\">&quot;write&quot;</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>You can access the metadata directly by calling <code class=\"docutils literal notranslate\"><span class=\"pre\">resource.config()</span></code> and reconstruct the resource with\n<code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;Resource</span> <span class=\"pre\">Type&gt;.from_config(config)</span></code>.</p>\n</section>\n<section id=\"terminating-modules-workers-or-clusters\">\n<h3>5. Terminating Modules, Workers, or Clusters<a class=\"headerlink\" href=\"#terminating-modules-workers-or-clusters\" title=\"Permalink to this heading\">\u00b6</a></h3>\n<p>When a remote object is no longer needed, it can be deallocated from the remote compute by calling\n<code class=\"docutils literal notranslate\"><span class=\"pre\">cluster.delete(obj_name)</span></code>. This will remove the object from the key-value store and free up the memory on the\nworker. A worker process can similarly be terminated with <code class=\"docutils literal notranslate\"><span class=\"pre\">cluster.delete(worker_name)</span></code>, terminating its activities\nand freeing its memory.</p>\n<p>To down a cluster when the task is complete and the resource is no longer needed, you can simply call\n<code class=\"docutils literal notranslate\"><span class=\"pre\">cluster.teardown()</span></code> or let the autostop handle the cluster termination.</p>\n</section>\n</section>\n<section id=\"moving-to-production\">\n<h2>Moving to Production<a class=\"headerlink\" href=\"#moving-to-production\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>A key advantage of using Runhouse is that the code developed locally has already been executing production-like on\nremote compute the entire time. This means research-to-production is an abstract checkpoint in development rather than\nan actual task to rewrite pipelines for production over different hardware/data.</p>\n<p>If your code is for a non-recurring task, then great, check your code into version control and you are already done. If\nyou are deploying a recurring job like recurring training, then simply move the Runhouse launching code into the\norchestrator or scheduler of your choice. You should not repackage ML code into orchestrator nodes and make\norchestrators your runtime. Instead, you should use orchestrators as minimal systems to schedule and observe your jobs,\nbut the jobs themselves will continue to be executed serverlessly with Runhouse from each node. This saves considerable\ntime upfront, setting up the first orchestrator to run in less than an hour (compared to multiple weeks in traditional\nML research-to-production).</p>\n<p>As an example, you might want to make the first task of your orchestrator pipeline to simply bring up the cluster and\ndispatch code to the new cluster. You can see that we are using the same underlying code (directly importing it from\na source file), and then reusing the object and cluster by name across steps.</p>\n<div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"nd\">@task</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">up_and_dispatch</span><span class=\"p\">():</span>\n      <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n          <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">Image</span><span class=\"p\">(</span><span class=\"s2\">&quot;base_setup&quot;</span><span class=\"p\">)</span>\n          <span class=\"o\">.</span><span class=\"n\">from_docker</span><span class=\"p\">(</span><span class=\"s2\">&quot;nvcr.io/nvidia/pytorch:23.10-py3&quot;</span><span class=\"p\">)</span>\n          <span class=\"o\">.</span><span class=\"n\">install_packages</span><span class=\"p\">([</span><span class=\"s2\">&quot;torch&quot;</span><span class=\"p\">])</span>\n      <span class=\"p\">)</span>\n      <span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">ondemand_cluster</span><span class=\"p\">(</span>\n            <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;rh-cluster&quot;</span><span class=\"p\">,</span>\n            <span class=\"n\">instance_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;A10G:1&quot;</span><span class=\"p\">,</span>\n            <span class=\"n\">provider</span><span class=\"o\">=</span><span class=\"s2\">&quot;aws&quot;</span><span class=\"p\">,</span>\n            <span class=\"n\">image</span><span class=\"o\">=</span><span class=\"n\">image</span><span class=\"p\">,</span>\n      <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">up_if_not</span><span class=\"p\">()</span>\n\n      <span class=\"kn\">from</span> <span class=\"nn\">my_code</span> <span class=\"kn\">import</span> <span class=\"n\">TorchTrainer</span>\n      <span class=\"n\">RemoteTrainer</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"p\">(</span><span class=\"n\">TorchTrainer</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"p\">)</span>\n      <span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">RemoteTrainer</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;remote-instance-of-trainer&#39;</span><span class=\"p\">)</span>\n\n<span class=\"nd\">@task</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">embed</span><span class=\"p\">():</span>\n      <span class=\"n\">cluster</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">cluster</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;rh-cluster&quot;</span><span class=\"p\">)</span>\n      <span class=\"n\">trainer</span> <span class=\"o\">=</span> <span class=\"n\">cluster</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s1\">&#39;remote-instance-of-trainer&#39;</span><span class=\"p\">)</span>\n      <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"o\">...</span>  <span class=\"c1\"># Load data</span>\n      <span class=\"n\">trainer</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n<p>Runhouse recommends creating a Docker container which fixes the environment, dependencies, and program code for\nproduction pipelines. There are significant benefits to containerization, rather than, for instance, worrying\nabout new breaking changes from package installation with PyPi. This is actually still unproblematic for additional\nfuture iteration or debug, since you still easily interactively layer on changes to the environment from local, even\nwhen you launch with the container.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/Identical+Dispatch+in+Production.jpg\"><img alt=\"Send code from research and production to compute\" class=\"align-center\" src=\"https://runhouse-tutorials.s3.amazonaws.com/Identical+Dispatch+in+Production.jpg\" style=\"width: 750px;\" /></a>\n</section>\n<section id=\"my-pipeline-is-in-production-what-s-next\">\n<h2>My Pipeline is in Production, What\u2019s Next?<a class=\"headerlink\" href=\"#my-pipeline-is-in-production-what-s-next\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Once in production, your ML pipelines will eventually experience some failures you need to debug. With Runhouse,\nengineers can easily and locally reproduce production runs, make changes to the underlying code, and push a change to\nthe codebase. There is no debugging through the orchestrator, and no need to rebuild and resubmit. Fruthermore, we find\nthat deploying with Runhouse has fewer errors to begin with, as the code has already been developed in a\nproduction-like environment.</p>\n<p>This also makes production-to-research a seamless process. Many teams loathe revisiting the research-to-production\nprocess, so when code is deployed to production, there is little appetite to make small incremental improvements to the\npipeline. With Runhouse, the pipeline is already running serverlessly, so incremental changes that are merged to the\nteam codebase are automatically reflected in the production pipeline once tested via normal development processes.</p>\n<p>There are other benefits to using Runhouse in production as you scale up usage. A few are included here:</p>\n<ul class=\"simple\">\n<li><p><strong>Shared services</strong>: You may want to deploy shared services like an embeddings endpoint, and have all pipelines call\nit by name as a live service <em>or</em> import the code from the underlying team repository and stand it up separately in\neach pipeline. Either way, if you every update or improve this shared service, all pipelines will receive the\ndownstream updates without any changes to the pipeline code.</p></li>\n<li><p><strong>Compute abstraction</strong>: As you add new resources to your pool, get credits from new clouds, or get new quota, if all\nusers are using Runhouse to allocate ephemeral compute, there is no need to update any code or configuration files at\nthe user level. The new resources are added by the platform team, and then automatically adopted by the full team.</p></li>\n<li><p><strong>Infrastructure Migrations</strong>: With Runhouse, your application code is entirely undecorated Python and the dispatch\nhappens to arbitrary compute. If you ever choose to abandon your existing orchestrator, cloud provider, or any other\ntool, you simply have to move a small amount of dispatch code and infrastructure code configuration.</p></li>\n<li><p><strong>Adopting Distributed Frameworks</strong>: Runhouse is a perfect complement to distributed frameworks, with some built-in\nabstractions that let you scale to multiple clusters or start using Ray clusters easily.</p></li>\n</ul>\n</section>\n</section>\n\n    <script type=\"text/x-thebe-config\">\n    {\n        requestKernel: true,\n        binderOptions: {\n            repo: \"binder-examples/jupyter-stacks-datascience\",\n            ref: \"master\",\n        },\n        codeMirrorConfig: {\n            theme: \"abcdef\",\n            mode: \"python\"\n        },\n        kernelOptions: {\n            name: \"python3\",\n            path: \"./.\"\n        },\n        predefinedOutput: true\n    }\n    </script>\n    <script>kernelName = 'python3'</script>", "metatags": "<meta name=\"generator\" content=\"Docutils 0.19: https://docutils.sourceforge.io/\" />\n", "rellinks": [["genindex", "General Index", "I", "index"], ["py-modindex", "Python Module Index", "", "modules"], ["runhouse-in-your-stack", "Working with Common Libraries and Tools", "N", "next"], ["tutorials/quick-start-den", "Den Quick Start", "P", "previous"]], "sourcename": "how-to-use-runhouse.rst.txt", "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">How to Use Runhouse</a><ul>\n<li><a class=\"reference internal\" href=\"#quick-start\">Quick Start</a></li>\n<li><a class=\"reference internal\" href=\"#access-to-compute\">Access to Compute</a></li>\n<li><a class=\"reference internal\" href=\"#start-your-project\">Start Your Project</a><ul>\n<li><a class=\"reference internal\" href=\"#define-compute\">1. Define Compute</a><ul>\n<li><a class=\"reference internal\" href=\"#starting-the-runhouse-server-daemon\">Starting the Runhouse Server Daemon</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#dispatch-your-code\">2. Dispatch Your Code</a></li>\n<li><a class=\"reference internal\" href=\"#execute-your-code-remotely\">3. Execute Your Code Remotely</a></li>\n<li><a class=\"reference internal\" href=\"#saving-and-loading\">4. Saving and Loading</a></li>\n<li><a class=\"reference internal\" href=\"#terminating-modules-workers-or-clusters\">5. Terminating Modules, Workers, or Clusters</a></li>\n</ul>\n</li>\n<li><a class=\"reference internal\" href=\"#moving-to-production\">Moving to Production</a></li>\n<li><a class=\"reference internal\" href=\"#my-pipeline-is-in-production-what-s-next\">My Pipeline is in Production, What\u2019s Next?</a></li>\n</ul>\n</li>\n</ul>\n", "display_toc": true, "page_source_suffix": ".rst", "globaltoc": "<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-cloud/\">Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-den/\">Den Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"how-to-use-runhouse/\">How to Use Runhouse</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"runhouse-in-your-stack/\">Working with Common Libraries and Tools</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Basics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-clusters/\">Clusters</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-modules/\">Functions and Modules</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-process/\">Processes</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-images/\">Images</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-folders/\">Folders</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-secrets/\">Secrets</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-resources/\">Resource Management</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Reference</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/python/\">Python API</a><ul>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/resource/\">Resource</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/function/\">Function</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/cluster/\">Cluster</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/image/\">Image</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/package/\">Package</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/module/\">Module</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/folder/\">Folder</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/secrets/\">Secrets</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/login/\">Login/Logout</a></li>\n</ul>\n</li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/cli/\">Command Line Interface</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Other Topics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/async/\">Asynchronous Programming</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"installation/\">Installation and Setup</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"debugging-logging/\">Debugging and Logging</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docker-setup/\">Docker: Cluster Setup</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docker-workflows/\">Docker: Dev and Prod Workflows</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"troubleshooting/\">Manual Setup and Troubleshooting</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"security-and-authentication/\">Security and Authentication</a></li>\n</ul>\n", "current_page_name": "how-to-use-runhouse", "sidebars": ["about.html", "navigation.html", "relations.html", "searchbox.html", "donate.html"], "customsidebar": null, "alabaster_version": "0.7.16", "alabaster_version_info": [0, 7, 16]}