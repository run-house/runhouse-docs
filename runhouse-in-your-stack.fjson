{"parents": [], "prev": {"link": "../how-to-use-runhouse/", "title": "How to Use Runhouse"}, "next": {"link": "../tutorials/api-clusters/", "title": "Clusters"}, "title": "Working with Common Libraries and Tools", "meta": {}, "body": "<section id=\"working-with-common-libraries-and-tools\">\n<h1>Working with Common Libraries and Tools<a class=\"headerlink\" href=\"#working-with-common-libraries-and-tools\" title=\"Permalink to this heading\">\u00b6</a></h1>\n<p>Runhouse is built to be extremely unopinionated and designed to work closely with familiar tools and libraries in the\nML ecosystem.</p>\n<section id=\"notebooks-and-ides-hosted-or-local\">\n<h2>Notebooks and IDEs (Hosted or Local)<a class=\"headerlink\" href=\"#notebooks-and-ides-hosted-or-local\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>ML engineers often lose the ability to develop locally when dataset sizes and the need for accelerated compute exceed\nthe capabilities of local hardware. Hosted notebooks have become a common solution for rapid iteration during the\nresearch phase, but they can fragment the development workflow and introduce a \u201cresearch-to-production\u201d gap that\ndoesn\u2019t exist in traditional software engineering.</p>\n<p>Runhouse advocates for defining ML programs and pipelines using standard Python code. Classes and functions should be\nportable, importable, testable, and managed with software best practices within a team repository. Code development is\nbest done in a traditional IDE, but Runhouse is flexible in how the code is executed interactively. You can launch\ncompute, dispatch code, and execute it in script form or use Python notebooks as interactive shells.</p>\n<p>With Runhouse, classes are remote objects that can be accessed through multi-threaded calls. We can instantiate a\nremote trainer class and launch training loops in one local thread, and establish a separate connection to the remote\nobject in another thread. This allows us to perform multiple tasks simultaneously with the same remote object. For\nexample, running training epochs, saving model checkpoints, and conducting test evaluations can be done from three\nasync calls, three scripts, or three notebook cells.</p>\n<p>We show how a LoRA Fine Tuner class can be launched from a notebook in\n<a class=\"reference external\" href=\"https://github.com/run-house/runhouse/tree/1b047c9b22839c212a1e2674407959e7e775f21b/examples/lora-example-with-notebook\">this example</a>.</p>\n</section>\n<section id=\"workflow-orchestrators-e-g-airflow-prefect-dagster-flyte-metaflow-argo\">\n<h2>Workflow orchestrators (e.g. Airflow, Prefect, Dagster, Flyte, Metaflow, Argo)<a class=\"headerlink\" href=\"#workflow-orchestrators-e-g-airflow-prefect-dagster-flyte-metaflow-argo\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Workflow orchestrators are excellent for monitoring, telemetry, fault tolerance, and scheduling, and we recommend\nusing them for these tasks. However, they shouldn\u2019t act as your application or runtime - ML development with\n\u201cnotebooks-plus-DAGs\u201d leads to poor reproducibility, bad debuggability, and slow research-to-production. Rather\nthan convert a Python application into an Airflow DAG to run training on a GPU, Runhouse lets you simply make an HTTP\ncall within an Airflow step to trigger a training function running as a service.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+WO+Runhouse.jpg\"><img alt=\"Fragmented research and production in separate compute environments\" class=\"align-center\" src=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+WO+Runhouse.jpg\" style=\"width: 650px;\" /></a>\n<p>By avoiding the need to repack ML code into pipelines, teams can significantly reduce research-to-production time and\nimprove debuggability. Runhouse ensures that the code committed to the team repository will execute reproducibly in\nproduction without additional translation, and that iteration loops will remain fast in production, whether for\ndebugging or further development. ML engineers can reproduce a failed production run locally by copying the dispatch\ncode, quickly debug and iterate, then push the changes. This approach is much faster than the traditional 20+ minute\ncycles required to rebuild and rerun orchestrator pipelines.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+W+Runhouse.jpg\"><img alt=\"Unified dispatch from notebooks and nodes with Runhouse\" class=\"align-center\" src=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+W+Runhouse.jpg\" style=\"width: 650px;\" /></a>\n<p>There are many clever patterns that Runhouse enables in conjunction with orchestrators that save time and money.</p>\n<ul class=\"simple\">\n<li><p>Reusing the same compute across multiple tasks while separating the steps in the orchestrator for clarity. For\ninstance, avoiding the I/O overhead of repeatedly writing/reading data for each step of an Argo/Kubeflow pipeline.</p></li>\n<li><p>Sharing a single service across multiple orchestrator pipelines. For instance, a single embeddings service can be\nused by multiple pipelines.</p></li>\n<li><p>Maintaining a single orchestrator, but dispatching each pipeline step to arbitrary clusters, regions, or even clouds.\nFor instance, do pre-processing on AWS, but GPU training on GCP where you have quota/credits.</p></li>\n<li><p>Catching and handling errors natively from the orchestrator node, since the orchestrator runtime is a Python-based\ndriver for the execution. For instance, on fail due to OOM, launch a larger box and rerun.</p></li>\n</ul>\n</section>\n<section id=\"distributed-frameworks-e-g-ray-spark-elixr\">\n<h2>Distributed frameworks (e.g. Ray, Spark, Elixr)<a class=\"headerlink\" href=\"#distributed-frameworks-e-g-ray-spark-elixr\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Runhouse is a perfect complement to distributed frameworks, letting you use these frameworks in a less disruptive way.</p>\n<p>Distributed frameworks are built to offload execution to different processes or nodes <em>within</em> their own cluster\nenvironments. Runhouse is focused on dispatching execution to compute resources <em>outside</em> Runhouse\u2019s own runtime (which\nis Python) and coordinating execution across different types of clusters. As an example, when using Ray with Runhouse,\nuse Runhouse to launch a cluster and send a function to the head node of a Ray cluster, where Ray will execute it as\nusual.</p>\n<p>This approach fixes some sharp edges of traditional distributed frameworks. First, because the local and remote compute\nenvironments are decoupled, there is no shared runtime that could fail if one part disconnects or experiences downtime,\nwhereas without Runhouse, an out-of-memory error in a node has a high chance of crashing the entire application.\nRunhouse also enables using of multiple clusters in a single application or sharing a cluster across multiple different\ncallers.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/Runhouse+and+Distributed+DSLs.jpg\"><img alt=\"Runhouse distributes from Python to a Ray Cluster (or Spark)\" class=\"align-center\" src=\"https://runhouse-tutorials.s3.amazonaws.com/Runhouse+and+Distributed+DSLs.jpg\" style=\"width: 650px;\" /></a>\n</section>\n<section id=\"serverless-frameworks-e-g-aws-lambda-google-cloud-functions-fireworks-modal\">\n<h2>Serverless frameworks (e.g. AWS Lambda, Google Cloud Functions, Fireworks, Modal)<a class=\"headerlink\" href=\"#serverless-frameworks-e-g-aws-lambda-google-cloud-functions-fireworks-modal\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Serverless frameworks enable on-the-fly service allocation, and similarly to Runhouse, abstract compute management\naway from engineers. However, they often require pre-packaging or command-line interface (CLI) launches outside of\nstandard Python environments. Runhouse, on the other hand, runs entirely within a Python interpreter, allowing it to\nextend the compute capabilities of existing Python applications. Very critically, Runhouse lets you <strong>allocate\nresources within your own infrastructure</strong>.</p>\n<p>Serverless solutions are a broad category, and many serverless solutions aren\u2019t suitable for ML workloads.\nFor instance, AWS Lambda struggles with large datasets, GPU-accelerated tasks, or long-running jobs. Runhouse can\noffload these tasks to ephemerally launched, but powerful compute that lasts until the job is done.</p>\n<p>Even when evaluating serverless solutions optimized for ML, it\u2019s essential to distinguish between those optimized for\ninference and Runhouse. For inference, you likely prioritize latency and cold start times, and typically execute on a\nfew limited types of hardware. For recurring training, you want better hardware heterogeneity, debuggability,\nstatefulness across epochs, and the ability to efficiently use compute; Runhouse is optimized for this.</p>\n</section>\n<section id=\"slurm-style-compute-interfaces-e-g-slurm-skypilot-mosaic-sagemaker-training\">\n<h2>Slurm-Style Compute Interfaces (e.g. Slurm, SkyPilot, Mosaic, SageMaker Training)<a class=\"headerlink\" href=\"#slurm-style-compute-interfaces-e-g-slurm-skypilot-mosaic-sagemaker-training\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>In this category of Slurm-style solutions, compute is allocated on the fly and scripts are used as entry points. For\nheavyweight jobs that are run manually, such as a research lab training a large language model over hundreds of GPUs,\nthis style of execution works quite well. However, for recurring enterprise ML use cases, there are several distinct\ndisadvantages that Runhouse addresses.</p>\n<ul class=\"simple\">\n<li><p>Limited control over execution flow, making it difficult to dispatch multiple stages or function calls to the same\ncompute resource (e.g., loading datasets, training, and evaluation).</p></li>\n<li><p>Weak fault tolerance due to the inability to catch and handle remote exceptions (all exception handling must occur\nwithin the script, leaving little recourse for issues like out-of-memory errors)</p></li>\n<li><p>Configuration sprawl as training scripts branch for each new method or experiment, and combinations of settings that\nwork together grow sparser and sparser.</p></li>\n</ul>\n<p>For elastic compute scenarios, Runhouse uses SkyPilot to allocate resources but goes beyond that by offering\n(re)deployment and execution management. This restores control over execution, adds fault tolerance, and allows all\ncompute configurations to be defined in code.</p>\n</section>\n</section>\n\n    <script type=\"text/x-thebe-config\">\n    {\n        requestKernel: true,\n        binderOptions: {\n            repo: \"binder-examples/jupyter-stacks-datascience\",\n            ref: \"master\",\n        },\n        codeMirrorConfig: {\n            theme: \"abcdef\",\n            mode: \"python\"\n        },\n        kernelOptions: {\n            name: \"python3\",\n            path: \"./.\"\n        },\n        predefinedOutput: true\n    }\n    </script>\n    <script>kernelName = 'python3'</script>", "metatags": "<meta name=\"generator\" content=\"Docutils 0.19: https://docutils.sourceforge.io/\" />\n", "rellinks": [["genindex", "General Index", "I", "index"], ["py-modindex", "Python Module Index", "", "modules"], ["tutorials/api-clusters", "Clusters", "N", "next"], ["how-to-use-runhouse", "How to Use Runhouse", "P", "previous"]], "sourcename": "runhouse-in-your-stack.rst.txt", "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Working with Common Libraries and Tools</a><ul>\n<li><a class=\"reference internal\" href=\"#notebooks-and-ides-hosted-or-local\">Notebooks and IDEs (Hosted or Local)</a></li>\n<li><a class=\"reference internal\" href=\"#workflow-orchestrators-e-g-airflow-prefect-dagster-flyte-metaflow-argo\">Workflow orchestrators (e.g. Airflow, Prefect, Dagster, Flyte, Metaflow, Argo)</a></li>\n<li><a class=\"reference internal\" href=\"#distributed-frameworks-e-g-ray-spark-elixr\">Distributed frameworks (e.g. Ray, Spark, Elixr)</a></li>\n<li><a class=\"reference internal\" href=\"#serverless-frameworks-e-g-aws-lambda-google-cloud-functions-fireworks-modal\">Serverless frameworks (e.g. AWS Lambda, Google Cloud Functions, Fireworks, Modal)</a></li>\n<li><a class=\"reference internal\" href=\"#slurm-style-compute-interfaces-e-g-slurm-skypilot-mosaic-sagemaker-training\">Slurm-Style Compute Interfaces (e.g. Slurm, SkyPilot, Mosaic, SageMaker Training)</a></li>\n</ul>\n</li>\n</ul>\n", "display_toc": true, "page_source_suffix": ".rst", "globaltoc": "<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-cloud/\">Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-den/\">Den Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"how-to-use-runhouse/\">How to Use Runhouse</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"runhouse-in-your-stack/\">Working with Common Libraries and Tools</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Basics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-clusters/\">Clusters</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-modules/\">Functions and Modules</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-process/\">Processes</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-images/\">Images</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-folders/\">Folders</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-secrets/\">Secrets</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-resources/\">Resource Management</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Reference</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/python/\">Python API</a><ul>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/resource/\">Resource</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/function/\">Function</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/cluster/\">Cluster</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/image/\">Image</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/package/\">Package</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/module/\">Module</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/folder/\">Folder</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/secrets/\">Secrets</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/login/\">Login/Logout</a></li>\n</ul>\n</li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/cli/\">Command Line Interface</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Other Topics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/async/\">Asynchronous Programming</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"installation/\">Installation and Setup</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"debugging-logging/\">Debugging and Logging</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docker-setup/\">Docker: Cluster Setup</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docker-workflows/\">Docker: Dev and Prod Workflows with Runhouse</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"security-and-authentication/\">Security and Authentication</a></li>\n</ul>\n", "current_page_name": "runhouse-in-your-stack", "sidebars": ["about.html", "navigation.html", "relations.html", "searchbox.html", "donate.html"], "customsidebar": null, "alabaster_version": "0.7.16", "alabaster_version_info": [0, 7, 16]}