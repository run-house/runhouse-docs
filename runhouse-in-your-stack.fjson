{"parents": [], "prev": {"link": "../how-to-use-runhouse/", "title": "How to Use Runhouse"}, "next": {"link": "../tutorials/api-clusters/", "title": "Clusters"}, "title": "Working with Common Libraries and Tools", "meta": {}, "body": "<section id=\"working-with-common-libraries-and-tools\">\n<h1>Working with Common Libraries and Tools<a class=\"headerlink\" href=\"#working-with-common-libraries-and-tools\" title=\"Permalink to this heading\">\u00b6</a></h1>\n<p>Runhouse is built to be extremely unopinionated and designed to work closely with familiar tools and libraries in the ML ecosystem.</p>\n<section id=\"notebooks-and-ides-hosted-or-local\">\n<h2>Notebooks and IDEs (Hosted or Local)<a class=\"headerlink\" href=\"#notebooks-and-ides-hosted-or-local\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>ML engineers often lose the ability to develop locally when dataset sizes and the need for accelerated compute exceed the capabilities of local hardware.\nHosted notebooks have become a common solution for rapid iteration during the research phase, but they can fragment the development workflow and introduce\na \u201cresearch-to-production\u201d gap that doesn\u2019t exist in traditional software engineering.</p>\n<p>Runhouse advocates for defining ML programs and pipelines using standard Python code. Classes and functions should be portable,\nimportable, testable, and managed with software best practices within a team repository. Code development is best done in a traditional IDE,\nbut Runhouse is flexible in how the code is executed interactively. You can launch compute, dispatch code,\nand execute it in script form or use Python notebooks as interactive shells.</p>\n<p>With Runhouse, classes are remote objects that can be accessed through multi-threaded calls.\nIf we instantiate a remote trainer class and launch training loops in one local thread, we can\nalso establish a separate connection to the remote object in another thread. This allows us to perform multiple tasks\nsimultaneously with the same remote object: for example, running training epochs, saving model checkpoints, and conducting test evaluations.\nThese tasks can be done from three async calls, three scripts, or three notebook cells.</p>\n<p>We show here how a LoRA Fine Tuner class can be launched from a notebook\nin <a class=\"reference external\" href=\"https://github.com/run-house/runhouse/tree/1b047c9b22839c212a1e2674407959e7e775f21b/examples/lora-example-with-notebook\">this example</a>.</p>\n</section>\n<section id=\"workflow-orchestrators-e-g-airflow-prefect-dagster-flyte-metaflow-argo\">\n<h2>Workflow orchestrators (e.g. Airflow, Prefect, Dagster, Flyte, Metaflow, Argo)<a class=\"headerlink\" href=\"#workflow-orchestrators-e-g-airflow-prefect-dagster-flyte-metaflow-argo\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Workflow orchestrators are excellent for monitoring, telemetry, fault tolerance, and scheduling, so we recommend using them for these tasks.\nHowever, they shouldn\u2019t act as your application or runtime. For example, instead of converting a Python application into an Airflow DAG to run\ntraining on a GPU, you simply make an HTTP call within an Airflow step to trigger a training function running as a service. ML development\nwith \u201cnotebooks-plus-DAGs\u201d leads to poor reproducibility, bad debuggability, and slow research-to-production.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+WO+Runhouse.jpg\"><img alt=\"Fragmented research and production in separate compute environments\" src=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+WO+Runhouse.jpg\" style=\"width: 650px;\" /></a>\n<p>By avoiding the need to repack ML code into pipelines, teams can significantly reduce research-to-production time and improve debuggability.\nRunhouse ensures that the code committed to the team repository will execute reproducibly in production without additional translation.\nAdditionally, iteration loops remain fast in production, whether for debugging or further development.\nML engineers can reproduce a failed production run locally by copying the dispatch code, quickly debug and iterate, then push the changes.\nThis approach is much faster than the traditional 20+ minute cycles required to rebuild and rerun orchestrator pipelines.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+W+Runhouse.jpg\"><img alt=\"Unified dispatch from notebooks and nodes with Runhouse\" src=\"https://runhouse-tutorials.s3.amazonaws.com/R2P+W+Runhouse.jpg\" style=\"width: 650px;\" /></a>\n<p>There are many clever patterns that Runhouse enables in conjunction with orchestrators that save time and money.</p>\n<ul class=\"simple\">\n<li><p>Reusing of the same compute across multiple tasks while separating the steps in the orchestrator for clarity. For instance, avoiding the I/O overhead of repeatedly writing/reading data for each step of an Argo/Kubeflow pipeline.</p></li>\n<li><p>Sharing a single service to be shared across multiple orchestrator pipelines. For instance, a single embeddings service can be used by multiple pipelines.</p></li>\n<li><p>Maintaining a single orchestrator, but dispatching each pipeline step to arbitrary clusters, regions, or even clouds. For instance, do pre-processing on AWS, but GPU training on GCP where you have quota/credits.</p></li>\n<li><p>Catching and handling errors natively from the orchestrator node, since the orchestrator runtime is a Python-based driver for the execution. For instance, on fail due to OOM, launch a larger box and rerun.</p></li>\n</ul>\n</section>\n<section id=\"distributed-frameworks-e-g-ray-spark-elixr\">\n<h2>Distributed frameworks (e.g. Ray, Spark, Elixr)<a class=\"headerlink\" href=\"#distributed-frameworks-e-g-ray-spark-elixr\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Runhouse is a perfect complement to distributed frameworks, letting you use these frameworks in a less disruptive way.</p>\n<p>Distributed frameworks are built to offload execution to different processes or nodes <em>within</em> their own cluster environments.\nRunhouse is focused on dispatching execution to compute resources <em>outside</em> Runhouse\u2019s own runtime (which is Python)\nand coordinating execution across different types of clusters.\nAs an example, when using Ray with Runhouse, you use Runhouse to launch a cluster and then send a function to the head node of a Ray cluster, where Ray will execute it as usual.</p>\n<p>This approach fixes some sharp edges of traditional distributed frameworks. First, because the local\nand remote compute environments are decoupled, so there is no shared runtime\nthat could fail if one part disconnects or experiences downtime, whereas without Runhouse, an out-of-memory\nerror in a node has a high chance of crashing the entire application. Runhouse also enables the use of multiple clusters in a single application,\nand also supports sharing a cluster across multiple different callers.</p>\n<a class=\"reference internal image-reference\" href=\"https://runhouse-tutorials.s3.amazonaws.com/Runhouse+and+Distributed+DSLs.jpg\"><img alt=\"Runhouse distributes from Python to a Ray Cluster (or Spark)\" src=\"https://runhouse-tutorials.s3.amazonaws.com/Runhouse+and+Distributed+DSLs.jpg\" style=\"width: 650px;\" /></a>\n</section>\n<section id=\"serverless-frameworks-e-g-aws-lambda-google-cloud-functions-fireworks-modal\">\n<h2>Serverless frameworks (e.g. AWS Lambda, Google Cloud Functions, Fireworks, Modal)<a class=\"headerlink\" href=\"#serverless-frameworks-e-g-aws-lambda-google-cloud-functions-fireworks-modal\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Serverless frameworks enable on-the-fly service allocation, and similarly to Runhouse, abstract compute management away from engineers.\nHowever, they often require pre-packaging or command-line interface (CLI) launches outside of\nstandard Python environments. Runhouse, on the other hand, runs entirely within a Python interpreter, allowing it to extend the\ncompute capabilities of existing Python applications. Very critically, Runhouse lets you <strong>allocate resources within your own infrastructure</strong>.</p>\n<p>Serverless solutions are a broad category, and many serverless solutions aren\u2019t suitable for ML workloads. For instance, AWS Lambda struggles with large datasets, GPU-accelerated tasks,\nor long-running jobs. Runhouse can offload these tasks to ephemerally launched, but powerful compute that lasts until the job is done.\nEven when evaluating serverless solutions optimized for ML, it\u2019s essential to distinguish between those optimized for inference and Runhouse.\nFor inference, you likely prioritize latency, cold start times and typically execute on a few limited types of hardware.\nBut if you are considering executing recurring training, for instance, Runhouse is significantly more optimized; you have better hardware heterogeneity,\ndebuggability, statefulness across epochs, and the ability to efficiently use compute.</p>\n</section>\n<section id=\"slurm-style-compute-interfaces-e-g-slurm-skypilot-mosaic-sagemaker-training\">\n<h2>Slurm-Style Compute Interfaces (e.g. Slurm, SkyPilot, Mosaic, SageMaker Training)<a class=\"headerlink\" href=\"#slurm-style-compute-interfaces-e-g-slurm-skypilot-mosaic-sagemaker-training\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>In this category of Slurm-style solutions, compute is allocated on the fly and scripts are used as entry points.\nFor heavyweight jobs that are run manually, such as a research lab training a large language\nmodel over hundreds of GPUs, this style of execution works quite well. However, for recurring enterprise ML use cases, there are several distinct disadvantages\nthat Runhouse fixes.</p>\n<ul class=\"simple\">\n<li><p>Limited control over execution flow, making it difficult to dispatch multiple stages or function calls to the same compute resource (e.g., loading datasets, training, and evaluation).</p></li>\n<li><p>Weak fault tolerance due to the inability to catch and handle remote exceptions (all exception handling must occur within the script, leaving little recourse for issues like out-of-memory errors)</p></li>\n<li><p>Configuration sprawl as training scripts branch for each new method or experiment, and combinations of settings that work together grow sparser and sparser.</p></li>\n</ul>\n<p>For elastic compute scenarios, Runhouse uses SkyPilot to allocate resources but goes beyond that by offering (re)deployment and execution management.\nThis restores control over execution, adds fault tolerance, and allows all compute configurations to be defined in code.</p>\n</section>\n</section>\n\n    <script type=\"text/x-thebe-config\">\n    {\n        requestKernel: true,\n        binderOptions: {\n            repo: \"binder-examples/jupyter-stacks-datascience\",\n            ref: \"master\",\n        },\n        codeMirrorConfig: {\n            theme: \"abcdef\",\n            mode: \"python\"\n        },\n        kernelOptions: {\n            name: \"python3\",\n            path: \"./.\"\n        },\n        predefinedOutput: true\n    }\n    </script>\n    <script>kernelName = 'python3'</script>", "metatags": "<meta name=\"generator\" content=\"Docutils 0.19: https://docutils.sourceforge.io/\" />\n", "rellinks": [["genindex", "General Index", "I", "index"], ["py-modindex", "Python Module Index", "", "modules"], ["tutorials/api-clusters", "Clusters", "N", "next"], ["how-to-use-runhouse", "How to Use Runhouse", "P", "previous"]], "sourcename": "runhouse-in-your-stack.rst.txt", "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Working with Common Libraries and Tools</a><ul>\n<li><a class=\"reference internal\" href=\"#notebooks-and-ides-hosted-or-local\">Notebooks and IDEs (Hosted or Local)</a></li>\n<li><a class=\"reference internal\" href=\"#workflow-orchestrators-e-g-airflow-prefect-dagster-flyte-metaflow-argo\">Workflow orchestrators (e.g. Airflow, Prefect, Dagster, Flyte, Metaflow, Argo)</a></li>\n<li><a class=\"reference internal\" href=\"#distributed-frameworks-e-g-ray-spark-elixr\">Distributed frameworks (e.g. Ray, Spark, Elixr)</a></li>\n<li><a class=\"reference internal\" href=\"#serverless-frameworks-e-g-aws-lambda-google-cloud-functions-fireworks-modal\">Serverless frameworks (e.g. AWS Lambda, Google Cloud Functions, Fireworks, Modal)</a></li>\n<li><a class=\"reference internal\" href=\"#slurm-style-compute-interfaces-e-g-slurm-skypilot-mosaic-sagemaker-training\">Slurm-Style Compute Interfaces (e.g. Slurm, SkyPilot, Mosaic, SageMaker Training)</a></li>\n</ul>\n</li>\n</ul>\n", "display_toc": true, "page_source_suffix": ".rst", "globaltoc": "<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-cloud/\">Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-den/\">Den Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"how-to-use-runhouse/\">How to Use Runhouse</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"runhouse-in-your-stack/\">Working with Common Libraries and Tools</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Basics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-clusters/\">Clusters</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-modules/\">Functions and Modules</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-process/\">Processes</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-images/\">Images</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-folders/\">Folders</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-secrets/\">Secrets</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-resources/\">Resource Management</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Reference</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/python/\">Python API</a><ul>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/resource/\">Resource</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/function/\">Function</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/cluster/\">Cluster</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/image/\">Image</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/package/\">Package</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/module/\">Module</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/folder/\">Folder</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/secrets/\">Secrets</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/login/\">Login/Logout</a></li>\n</ul>\n</li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/cli/\">Command Line Interface</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Other Topics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/async/\">Asynchronous Programming</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"installation/\">Installation and Setup</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"debugging-logging/\">Debugging and Logging</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docker-setup/\">Docker: Cluster Setup</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docker-workflows/\">Docker: Dev and Prod Workflows with Runhouse</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"troubleshooting/\">Manual Setup and Troubleshooting</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"security-and-authentication/\">Security and Authentication</a></li>\n</ul>\n", "current_page_name": "runhouse-in-your-stack", "sidebars": ["about.html", "navigation.html", "relations.html", "searchbox.html", "donate.html"], "customsidebar": null, "alabaster_version": "0.7.16", "alabaster_version_info": [0, 7, 16]}