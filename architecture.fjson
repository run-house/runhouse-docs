{"parents": [], "prev": {"link": "../tutorials/async/", "title": "Asynchronous Programming"}, "next": {"link": "../installation/", "title": "Installation and Setup"}, "title": "Architecture Overview", "meta": {}, "body": "<section id=\"architecture-overview\">\n<h1>Architecture Overview<a class=\"headerlink\" href=\"#architecture-overview\" title=\"Permalink to this heading\">\u00b6</a></h1>\n<p>Runhouse is a Python library that allows any application to flexibly and powerfully utilize remote compute\ninfrastructure by deploying and calling remote services on the fly. It is principally designed for Machine\nLearning-style workloads (online, offline, training, and inference), where the need for heterogeneous\nremote compute is frequent and flexibility is paramount to minimize costs.</p>\n<p>The basic hypothesis is that incorporating dynamic compute into the runtime of an application, like workflow\norchestrators (Airflow, Prefect) or distributed libraries (Ray, Spark) is far more disruptive and less flexible at\nevery level (dev workflow, debugging, DevOps, infra) than calling a remote service on the compute within the\napplication. Calling a function or class as a remote service is a common pattern (e.g. microservices, Temporal),\nbut divides the code into multiple applications which multiplies the DevOps overhead, each having their own\nconfiguration, automation, scaling, etc. Runhouse achieves the best of both: limitless compute dynamism and\nflexibility in Python applications, without disrupting the runtime or cleaving the application, by offloading\nfunctions and classes to remote compute as services on the fly.</p>\n</section>\n<section id=\"why\">\n<h1>Why?<a class=\"headerlink\" href=\"#why\" title=\"Permalink to this heading\">\u00b6</a></h1>\n<p>This solves a few major problems for AI teams:\n1. <strong>Cost</strong>: The added flexibility Runhouse introduces to allocate compute only while needed, right-size instances based</p>\n<blockquote>\n<div><p>on the workload, reach across regions or clouds for lower costs, and share compute and services across tasks\ngenerally leads to large cost savings, on the order of 50-75% depending on the workload.</p>\n</div></blockquote>\n<ol class=\"arabic simple\" start=\"2\">\n<li><dl class=\"simple\">\n<dt><strong>Development at scale</strong>: Powerful hardware such as GPUs or distributed clusters (Spark, Ray) can be hugely</dt><dd><p>disruptive, requiring all development, debugging, automation, and deployment to occur on their runtime. Ray, Spark,\nor PyTorch distributed users for example must be tunneled into the head node at all times for development, leading\nto a proliferation of hosted notebook services as a stop-gap. Runhouse allows Python to orchestrate to these\nsystems remotely, returning the development workflow and operations to that of standard Python. Teams using Runhouse\ncan abandon hosted development notebooks and sandboxes entirely, again saving considerable cost and\nresearch-to-production time.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt><strong>Infrastructure overhead</strong>: Runhouse thoughtfully captures infrastructure concerns in code, providing a clear</dt><dd><p>contract between the application and infrastructure, and saving ML teams from learning all the infra, networking,\nsecurity, and DevOps underneath.</p>\n</dd>\n</dl>\n</li>\n</ol>\n<section id=\"high-level-flow\">\n<h2>High-level Flow<a class=\"headerlink\" href=\"#high-level-flow\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>The basic flow of how Runhouse offloads function and classes as services is as follows. You can follow along with this\nannotated code snippet:</p>\n<blockquote>\n<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">runhouse</span> <span class=\"k\">as</span> <span class=\"nn\">rh</span>\n\n<span class=\"c1\"># [1] and [2]</span>\n<span class=\"n\">gpu</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">cluster</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;rh-a10x&quot;</span><span class=\"p\">,</span> <span class=\"n\">instance_type</span><span class=\"o\">=</span><span class=\"s2\">&quot;A10G:1&quot;</span><span class=\"p\">,</span> <span class=\"n\">provider</span><span class=\"o\">=</span><span class=\"s2\">&quot;aws&quot;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">up_if_not</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># [3]</span>\n<span class=\"n\">sd_env</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">env</span><span class=\"p\">(</span><span class=\"n\">reqs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;torch&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;transformers&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;diffusers&quot;</span><span class=\"p\">],</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;sd_generate&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">remote_sd_generate</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">sd_generate</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">gpu</span><span class=\"p\">,</span> <span class=\"n\">worker</span><span class=\"o\">=</span><span class=\"n\">sd_env</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># [4]</span>\n<span class=\"n\">imgs</span> <span class=\"o\">=</span> <span class=\"n\">remote_sd_generate</span><span class=\"p\">(</span><span class=\"s2\">&quot;A hot dog made out of matcha.&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">imgs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># [5]</span>\n<span class=\"n\">remote_sd_generate</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">()</span>\n<span class=\"n\">sd_upsampler</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;/my_username/sd_upsampler&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">high_res_imgs</span> <span class=\"o\">=</span> <span class=\"n\">sd_upsampler</span><span class=\"p\">(</span><span class=\"n\">imgs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># [6]</span>\n<span class=\"n\">gpu</span><span class=\"o\">.</span><span class=\"n\">down</span><span class=\"p\">()</span>\n</pre></div>\n</div>\n</div></blockquote>\n<ol class=\"arabic\">\n<li><dl class=\"simple\">\n<dt><strong>Specify and/or Allocate Compute</strong>: Runhouse can allocate compute to the application on the fly, either by</dt><dd><p>utilizing an existing VM or Ray cluster, or allocating one fresh using local cloud or K8s credentials. The\n<cite>rh.cluster</cite> constructor is generally used to specify and interact with remote compute, including bringing it up\nif necessary (<cite>cluster.up_if_not()</cite>).</p>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt><strong>Starting the Runhouse Server Daemon</strong>: If not already running, the client will start the Runhouse API server daemon</dt><dd><p>on the compute and form a network connection (either over SSH or HTTP/S). Dependencies can be specified to be\ninstalled before starting the daemon.</p>\n<blockquote>\n<div><ol class=\"loweralpha simple\">\n<li><dl class=\"simple\">\n<dt>The daemon can be thought of as a \u201cPython object server\u201d, holding key-value pairs of names and Python</dt><dd><p>objects in memory, and exposing an HTTP API to call methods on those objects by name.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt>The objects are held in a single default worker process by default, but can be sent to other worker</dt><dd><p>processes, including on other nodes in the cluster, to achieve powerful parallelism out of the box.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt>If I call GET <a class=\"reference external\" href=\"http://myserver:32300/my_object/my_method\">http://myserver:32300/my_object/my_method</a>, the daemon will look up the object named</dt><dd><p>\u201cmy_object\u201d, issue an instruction for its worker to call the method \u201cmy_method\u201d on it, and\nreturn the result.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt>The HTTP server and workers can handle thousands of concurrent calls per second, and have similar latency</dt><dd><p>under simple conditions to Flask.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt>New workers can be constructed with <cite>rh.env</cite>, which specifies the details of the Python environment</dt><dd><p>(packages, environment variables) in which the process will be constructed. By default workers live\nin the same Python environment as the daemon, but can also be started in a conda environment or a\nseparate node.</p>\n</dd>\n</dl>\n</li>\n</ol>\n</div></blockquote>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt><strong>Deploying Functions or Classes</strong>: The user specifies a function or class to be deployed to the remote compute</dt><dd><p>using the <cite>rh.function</cite> or <cite>rh.module</cite> constructors (or by subclassing <cite>rh.Module</cite>), and calling\n<cite>remote_obj = my_obj.to(my_cluster, worker=my_env)</cite>. The Runhouse client library extracts the path, module name,\nand importable name from the function or class. If the function or class is defined in local code, the repo or\npackage is rsynced onto the cluster. An instruction with the import path is sent to the cluster to\nconstruct the function or class in a particular worker and assign it a name in the key-value store, perhaps\noverwriting an existing object.</p>\n</dd>\n</dl>\n</li>\n<li><dl>\n<dt><strong>Calling the Function or Class</strong>: After deploying the function, class, or object into the server, the Runhouse</dt><dd><p>Python client returns a local callable stub which behaves like the original object, but forwards method calls\nover HTTP to the remote object on the cluster.</p>\n<blockquote>\n<div><ol class=\"loweralpha simple\">\n<li><dl class=\"simple\">\n<dt>If a stateful instance of a class is desired, an <cite>__init__</cite> method can be called on the remote class to</dt><dd><p>instantiate a new remote object from the class and assign it a name.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt>If arguments are passed to the method, they\u2019re serialized with cloudpickle and sent with the HTTP request.</dt><dd><p>Serializing code, such as functions, classes, or dataclasses, is strongly discouraged, as it can lead to\nversioning mismatch errors between local and remote package versions.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt>From here on, you can think of Runhouse as facilitating</dt><dd><p>regular object-oriented programming but with the objects living remotely, maybe in a different cluster,\nregion, or cloud than the local code.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt>Python behavior like async, exceptions, printing, and logging are all preserved across remote calls, but</dt><dd><p>can be disabled or controlled if desired.</p>\n</dd>\n</dl>\n</li>\n</ol>\n</div></blockquote>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt><strong>Saving and Loading</strong>: The Runhouse client can save and load objects to and from the local filesystem, or to a</dt><dd><p>remote metadata store. This allows for easy sharing of clusters and services across users and environments,\nand for versioning and rollback of objects. The metadata store can be accessed from any Python interpreter,\nand is backed by a management API to view and manage all resources.</p>\n</dd>\n</dl>\n</li>\n<li><dl class=\"simple\">\n<dt><strong>Terminating Modules, Workers, or Clusters</strong>: When a remote object is no longer needed, it can be deallocated from</dt><dd><p>the remote compute by calling <cite>cluster.delete(obj_name)</cite>. This will remove the object from the key-value store and\nfree up the memory on the worker. A full worker can similarly be terminated with <cite>cluster.delete(worker_name)</cite>. A\ncluster can be terminated with <cite>cluster.down()</cite>.</p>\n</dd>\n</dl>\n</li>\n</ol>\n</section>\n<section id=\"comparing-to-other-systems\">\n<h2>Comparing to other systems<a class=\"headerlink\" href=\"#comparing-to-other-systems\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Runhouse\u2019s APIs bear similarity to other systems, so it\u2019s helpful to compare and contrast. In many cases,\nRunhouse is not a replacement for these systems, but rather a complement or extension. In others, you may be able\nto replace your usage of the other system entirely with Runhouse.</p>\n<p><em>Distributed frameworks</em> like Ray, Spark, or Elixr make it possible to offload execution onto separate\ncompute, like a different process or node within a their cluster runtime. Runhouse\ncan be seen as similar, but with the crucial distinction of dispatching execution to compute <em>outside</em> of its own\nruntime (which is just Python) or orchestrating <em>between</em> clusters (even of different types).\nFor this reason, it has no other runtime to setup than Python itself, can be used to orchestrate your distributed code so you\ncan use your Ray or Spark clusters less disruptively within your stack (e.g. sending a function which uses\nRay over to the head node of the Ray cluster, where the Ray will execute as usual).</p>\n<p>&lt;diagram showing Ray calling actors within the cluster, Runhouse calling outside into other processes, VMs,\nRay clusters, K8s clusters, clouds&gt;</p>\n<p>This also fixes some other sharp\nedges with such systems which significantly reduces costs, such as utilizing more than one cluster in an application,\nor sharing a cluster between multiple callers. Is also means the local and remote compute are largely decoupled,\nwith no shared runtime which will break if one disconnects or goes down.</p>\n<p><em>Workflow orchestrators</em> like Airflow, Prefect, Dagster, Flyte, Metaflow, or Argo can allocate heterogeneous compute\non the the fly, but act as the runtime itself for the program and only support certain pre-defined and highly\nconstrained DAGs. By allocating services Runhouse allows for arbitrary control flow and utilization of remote\nhardware, making Python itself the orchestrator.\nFor example, with Runhouse it\u2019s easy to allocate small compute to start a training, but if the training fails due to OOM\nrestart it with a slightly larger box. Other compute flexibility like multi-region or multi-cloud which other\norchestrators struggle with are trivial for Runhouse.</p>\n<p>&lt;diagram showing Airflow as a sequence of nodes, and Runhouse as a call within Python to an outside node&gt;</p>\n<p>Generally, workflow orchestrators are built to be good at monitoring, telemetry, fault-tolerance, and scheduling, so\nwe recommend using one strictly for those features and using Runhouse within your pipeline nodes for the heterogeneous\ncompute and remote execution. You can also save a lot of money by reusing compute across multiple nodes or reusing\nservices across multiple pipelines with Runhouse, which is generally not possible with workflow orchestrators.</p>\n<p>&lt;diagram showing an Airflow pipeline calling multiple steps on one cluster, and another pipeline calling a service\ncreated by the first&gt;</p>\n<p><em>Serverless frameworks</em> like Modal or AWS Lambda allow for the allocation of services on the fly, but within a well\ndefined sandbox, and not strictly from within regular Python - they require specific pre-packaging or CLI launch\ncommands outside Python. Runhouse runs fully in a Python interpreter so it can extend the compute power of practically\nany existing Python application, and allocates services inside your own compute, wherever that may be. We may even\nsupport serverless systems as compute backends in the future.</p>\n<p>&lt;diagram showing Modal creating new services within their own compute, and Runhouse within EC2, K8s, GCP, etc.&gt;</p>\n<p><em>Infrastructure in code</em> tools like SkyPilot or Pulumi allocate compute on the fly, but can\u2019t utilize it instantly\nto offload execution within the application (though you could call a predefined script entrypoint or API\nendpoint). Runhouse uses SkyPilot to allocate compute, but is vertically integrated to be able\nto perform allocation, (re)deployment, and management of a new service all in Python so the new compute can be used\ninstantly within the existing application. It also doesn\u2019t need to perform allocation to create new services -\nit can use existing compute or static VMs.</p>\n<p>&lt;diagram showing SkyPilot running a CLI command on the remote compute and getting back logs, and Runhouse using\nSkyPilot to launch new compute, send a function to it, and call it&gt;</p>\n<p><em>GPU/Accelerator dispatch</em> systems like PyTorch, Jax, or Mojo give the ability to offload computation to a local GPU or\nTPU. Runhouse does not have this capability, but can offload a function or class to a remote instance with an\naccelerator, which can then itself use PyTorch or Jax (and maybe one day Mojo) to use the accelerator.</p>\n<p>&lt;diagram showing PyTorch dispatching to the GPU within the node, Runhouse dispatching to a remote node which then uses\nPyTorch to dispatch to the GPU&gt;</p>\n</section>\n<section id=\"saving-loading-and-sharing\">\n<h2>Saving, Loading, and Sharing<a class=\"headerlink\" href=\"#saving-loading-and-sharing\" title=\"Permalink to this heading\">\u00b6</a></h2>\n<p>Runhouse resources (clusters, functions, modules, environments) can be saved, shared, and reused based on a compact\nJSON metadata signature. This allows for easy sharing of clusters and services across users and environments, which\ncan often lead to massive cost savings. The <a class=\"reference external\" href=\"https://www.run.house/dashboard\">Den</a> hosted metadata store can be\naccessed via HTTP API or from any Python interpreter with a Runhouse token (either in <cite>~/.rh/config.yaml</cite> or an\n<cite>RH_TOKEN</cite> environment variable) like so:</p>\n<blockquote>\n<div><div class=\"highlight-python notranslate\"><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">runhouse</span> <span class=\"k\">as</span> <span class=\"nn\">rh</span>\n\n<span class=\"n\">remote_func</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"o\">=</span><span class=\"n\">my_func</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">my_cluster</span><span class=\"p\">,</span> <span class=\"n\">worker</span><span class=\"o\">=</span><span class=\"n\">my_env</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;my_function&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Save to Den</span>\n<span class=\"n\">remote_func</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Reload the function and invoke it remotely on the cluster</span>\n<span class=\"n\">remote_func</span> <span class=\"o\">=</span> <span class=\"n\">rh</span><span class=\"o\">.</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&quot;/my_username/my_function&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">remote_func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Share the function with another user, giving them access to call or modify the resource</span>\n<span class=\"n\">remote_func</span><span class=\"o\">.</span><span class=\"n\">share</span><span class=\"p\">(</span><span class=\"s2\">&quot;user_a@gmail.com&quot;</span><span class=\"p\">,</span> <span class=\"n\">access_level</span><span class=\"o\">=</span><span class=\"s2\">&quot;write&quot;</span><span class=\"p\">)</span>\n</pre></div>\n</div>\n</div></blockquote>\n<p>You can access the metadata directly by calling <cite>resource.config()</cite> and reconstruct the resource with\n<cite>&lt;Resource Type&gt;.from_config(config)</cite>.</p>\n</section>\n</section>\n\n    <script type=\"text/x-thebe-config\">\n    {\n        requestKernel: true,\n        binderOptions: {\n            repo: \"binder-examples/jupyter-stacks-datascience\",\n            ref: \"master\",\n        },\n        codeMirrorConfig: {\n            theme: \"abcdef\",\n            mode: \"python\"\n        },\n        kernelOptions: {\n            name: \"python3\",\n            path: \"./.\"\n        },\n        predefinedOutput: true\n    }\n    </script>\n    <script>kernelName = 'python3'</script>", "metatags": "<meta name=\"generator\" content=\"Docutils 0.19: https://docutils.sourceforge.io/\" />\n", "rellinks": [["genindex", "General Index", "I", "index"], ["py-modindex", "Python Module Index", "", "modules"], ["installation", "Installation and Setup", "N", "next"], ["tutorials/async", "Asynchronous Programming", "P", "previous"]], "sourcename": "architecture.rst.txt", "toc": "<ul>\n<li><a class=\"reference internal\" href=\"#\">Architecture Overview</a></li>\n<li><a class=\"reference internal\" href=\"#why\">Why?</a><ul>\n<li><a class=\"reference internal\" href=\"#high-level-flow\">High-level Flow</a></li>\n<li><a class=\"reference internal\" href=\"#comparing-to-other-systems\">Comparing to other systems</a></li>\n<li><a class=\"reference internal\" href=\"#saving-loading-and-sharing\">Saving, Loading, and Sharing</a></li>\n</ul>\n</li>\n</ul>\n", "display_toc": true, "page_source_suffix": ".rst", "globaltoc": "<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Getting Started</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-cloud/\">Cloud Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-local/\">Local Quick Start</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/quick-start-den/\">Den Quick Start</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Basics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-clusters/\">Clusters</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-modules/\">Functions and Modules</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-envs/\">Envs and Packages</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-folders/\">Folders</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-secrets/\">Secrets</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/api-resources/\">Resource Management</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">API Reference</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/python/\">Python API</a><ul>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/resource/\">Resource</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/function/\">Function</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/cluster/\">Cluster</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/env/\">Env</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/package/\">Package</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/module/\">Module</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/folder/\">Folder</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/blob/\">Blob</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/file/\">File</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/secrets/\">Secrets</a></li>\n<li class=\"toctree-l2\"><a class=\"reference internal\" href=\"api/python/login/\">Login/Logout</a></li>\n</ul>\n</li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"api/cli/\">Command Line Interface</a></li>\n</ul>\n<p class=\"caption\" role=\"heading\"><span class=\"caption-text\">Other Topics</span></p>\n<ul>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tutorials/async/\">Asynchronous Programming</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"architecture/\">Architecture Overview</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"architecture/#why\">Why?</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"installation/\">Installation and Setup</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"debugging-logging/\">Debugging and Logging</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"docker/\">Docker Runtime Env</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"troubleshooting/\">Manual Setup and Troubleshooting</a></li>\n<li class=\"toctree-l1\"><a class=\"reference internal\" href=\"security-and-authentication/\">Security and Authentication</a></li>\n</ul>\n", "current_page_name": "architecture", "sidebars": ["about.html", "navigation.html", "relations.html", "searchbox.html", "donate.html"], "customsidebar": null, "alabaster_version": "0.7.16", "alabaster_version_info": [0, 7, 16]}